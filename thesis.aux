\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\citation{Sommerfeld2018-zl}
\@writefile{toc}{\contentsline {chapter}{Abstract}{}{Doc-Start}}
\@writefile{toc}{\contentsline {chapter}{Acknowledgments}{i}{chapter*.1}}
\citation{Bowring2019-fc}
\citation{BOWRING2019116187}
\@writefile{toc}{\contentsline {chapter}{Declarations}{ii}{chapter*.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:introduction_chapter}{{1}{1}{Introduction}{chapter.1}{}}
\citation{Hong2019-qr,Ioannidis2005-sy,Wager2009-gm}
\citation{Lund2005-sf}
\citation{Skudlarski1999-ao}
\citation{Woolrich2001-tk}
\citation{Carp2013-cm}
\citation{Carp2012-ph}
\citation{Benjamini1995-yy}
\citation{Bennett2009-fh}
\citation{Eklund2016-ak}
\citation{Cox2017-wr}
\citation{Flandin2019-qx,Mueller2017-pn,Cox2017-ys}
\citation{Woo2014-ji}
\citation{Rozeboom1960-dp}
\citation{Meehl1967-ij}
\citation{Gonzalez-Castillo2012-do}
\citation{Bowring2019-fc}
\citation{Sommerfeld2018-zl}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{7}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:background}{{2}{7}{Background}{chapter.2}{}}
\citation{Anatomy1918-do}
\citation{Anatomy1918-do}
\citation{Azevedo2009-qj}
\citation{Adelman1987-hs,Mohamed2014-gl}
\citation{Lopez-Munoz2006-zk}
\citation{Diamond1964-cu,Bennett1964-vx}
\citation{Stacey2008-qx}
\citation{Kalia2013-bv}
\citation{Fox2015-ds}
\citation{Sperling2014-sy,McEvoy2009-zx}
\citation{Matthews2006-sl}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}The Study of Brain Function}{8}{section.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces An illustration showing the arteries at the base of the human brain. Reprinted from \citet  *{Anatomy1918-do}.\relax }}{8}{figure.caption.4}}
\citation{Hargreaves2012-dz}
\citation{Mechelli2005-dn}
\citation{Alexander2007-ut,Soares2013-mh}
\citation{Lee2013-kn}
\citation{Glover2011-at}
\citation{Smith2009-dm,Lee2012-di,Moussa2012-bl}
\citation{Brookes2011-cj,Fomina2015-ha}
\citation{Parker_Jones2017-ld,Tavor2016-pd}
\citation{Ogawa1990-it}
\citation{Bandettini1992-jt,Kwong1992-uq,Ogawa1992-af}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Blood-oxygen-level-dependant (BOLD) functional Magnetic Resonance Imaging (fMRI)}{10}{section.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Physiology of the BOLD Response}{11}{subsection.2.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces A schematic of the interaction between neurons. Image reused from Khan Academy.}}{11}{figure.caption.5}}
\citation{Buxton2012-ly}
\citation{Buxton2012-ly}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces The current BOLD signal model. In the presence of a stimulus, changes in biological parameters such as CBF, CBV and CMRO$_{2}$ influence the final observed BOLD response. Reprinted from \citet  *{Buxton2012-ly}, with permission from Elsevier.\relax }}{12}{figure.caption.6}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Task-based functional Magnetic Resonance Imaging (t-fMRI)}{12}{section.2.3}}
\newlabel{sec:t-fMRI}{{2.3}{12}{Task-based functional Magnetic Resonance Imaging (t-fMRI)}{section.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces The stimulus onset timings (blue) and expected response (red) for a task paradigm using a block design, where participants look at animal photos (top half), and a task paradigm using an event-related design, where participants are given a mild electric shock (bottom half).\relax }}{13}{figure.caption.7}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:paradigm}{{2.4}{13}{The stimulus onset timings (blue) and expected response (red) for a task paradigm using a block design, where participants look at animal photos (top half), and a task paradigm using an event-related design, where participants are given a mild electric shock (bottom half).\relax }{figure.caption.7}{}}
\citation{Cox1996-nu}
\citation{Jenkinson2012-wh}
\citation{Penny2011-uk}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Overview of Analysis Pipeline}{14}{section.2.4}}
\citation{Smith2002-vw}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Preprocessing}{15}{section.2.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Brain Extraction}{15}{subsection.2.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Distortion Correction}{16}{subsection.2.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Distortion correction applied to functional data with the use of a field map. While lost signal can not be recovered, the correction has vastly improved distorted regions in the frontal lobe. Functional data and field map images reprinted from the \textit  {fMRIB Graduate Programme} lecture notes, with the kind permission of Mark Jenkinson.}}{16}{figure.caption.8}}
\newlabel{fig:Distortion_Correction}{{2.5}{16}{Distortion correction applied to functional data with the use of a field map. While lost signal can not be recovered, the correction has vastly improved distorted regions in the frontal lobe. Functional data and field map images reprinted from the \textit {fMRIB Graduate Programme} lecture notes, with the kind permission of Mark Jenkinson}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Slice Timing Correction}{16}{subsection.2.5.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.4}Realignment}{17}{subsection.2.5.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.5}Coregistration}{18}{subsection.2.5.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.6}Spatial Normalization}{18}{subsection.2.5.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.7}Spatial Smoothing}{18}{subsection.2.5.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.8}Temporal Filtering}{19}{subsection.2.5.8}}
\newlabel{sec:temporal_filtering}{{2.5.8}{19}{Temporal Filtering}{subsection.2.5.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Showing the effect of high-pass filtering on one voxel's BOLD time series data. The high-pass filter has removed the slow drift seen in the raw BOLD signal on the left. Figure adapted from \textit  {fMRIB Graduate Programme} lecture notes, with the kind permission of Mark Jenkinson.\relax }}{20}{figure.caption.9}}
\newlabel{fig:High_Pass}{{2.6}{20}{Showing the effect of high-pass filtering on one voxel's BOLD time series data. The high-pass filter has removed the slow drift seen in the raw BOLD signal on the left. Figure adapted from \textit {fMRIB Graduate Programme} lecture notes, with the kind permission of Mark Jenkinson.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.9}Grand Mean Scaling}{20}{subsection.2.5.9}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Modelling of t-fMRI data with the General Linear Model}{20}{section.2.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}The GLM Set-up}{21}{subsection.2.6.1}}
\newlabel{sec:GLM}{{2.6.1}{21}{The GLM Set-up}{subsection.2.6.1}{}}
\newlabel{eq:GLM}{{2.1}{21}{The GLM Set-up}{equation.2.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Estimating the Parameters with Ordinary Least Squares (OLS)}{21}{subsection.2.6.2}}
\newlabel{sec:OLS}{{2.6.2}{21}{Estimating the Parameters with Ordinary Least Squares (OLS)}{subsection.2.6.2}{}}
\newlabel{eq:OLS_errors}{{2.2}{21}{Estimating the Parameters with Ordinary Least Squares (OLS)}{equation.2.6.2}{}}
\citation{Woolrich2001-tk}
\newlabel{eq:OLS_cost}{{2.3}{22}{Estimating the Parameters with Ordinary Least Squares (OLS)}{equation.2.6.3}{}}
\newlabel{eq:OLS_estimates}{{2.4}{22}{Estimating the Parameters with Ordinary Least Squares (OLS)}{equation.2.6.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}Prewhitening}{22}{subsection.2.6.3}}
\newlabel{sec:prewhitening}{{2.6.3}{22}{Prewhitening}{subsection.2.6.3}{}}
\newlabel{eq:correlated_errors}{{2.5}{22}{Prewhitening}{equation.2.6.5}{}}
\newlabel{eq:updated_GLM}{{2.6}{22}{Prewhitening}{equation.2.6.6}{}}
\newlabel{eq:updated_error_cov}{{2.7}{22}{Prewhitening}{equation.2.6.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.4}Estimating the Variance}{23}{subsection.2.6.4}}
\newlabel{eq:variance_estimator}{{2.8}{23}{Estimating the Variance}{equation.2.6.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.5}Inference with Null-Hypothesis Significance Testing}{23}{subsection.2.6.5}}
\newlabel{sec:NHST}{{2.6.5}{23}{Inference with Null-Hypothesis Significance Testing}{subsection.2.6.5}{}}
\newlabel{eq:contrast_distribution}{{2.9}{23}{Inference with Null-Hypothesis Significance Testing}{equation.2.6.9}{}}
\newlabel{eq:contrast_minus_param_distribution}{{2.10}{23}{Inference with Null-Hypothesis Significance Testing}{equation.2.6.10}{}}
\newlabel{eq:t_statistic}{{2.11}{24}{Inference with Null-Hypothesis Significance Testing}{equation.2.6.11}{}}
\newlabel{eq:f_stat_contrast}{{2.12}{24}{Inference with Null-Hypothesis Significance Testing}{equation.2.6.12}{}}
\newlabel{eq:f_statistic}{{2.13}{24}{Inference with Null-Hypothesis Significance Testing}{equation.2.6.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.6}First-Level (Subject-Level) Analysis}{24}{subsection.2.6.6}}
\newlabel{sec:first_level}{{2.6.6}{24}{First-Level (Subject-Level) Analysis}{subsection.2.6.6}{}}
\citation{Friston1998-jl}
\citation{Goutte2000-vd}
\citation{Lindquist2009-fs}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.7}Second-Level (Group-Level) Analysis}{25}{subsection.2.6.7}}
\newlabel{sec:group_level}{{2.6.7}{25}{Second-Level (Group-Level) Analysis}{subsection.2.6.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Showing how the predicted response for the animal photo task in Figure \ref  {fig:paradigm} is created by convolution of the onset timing function with the HRF.\relax }}{26}{figure.caption.10}}
\newlabel{fig:convolution}{{2.7}{26}{Showing how the predicted response for the animal photo task in Figure \ref {fig:paradigm} is created by convolution of the onset timing function with the HRF.\relax }{figure.caption.10}{}}
\newlabel{eq:theoretical_group_GLM}{{2.14}{26}{Second-Level (Group-Level) Analysis}{equation.2.6.14}{}}
\newlabel{eq:group_errors}{{2.15}{27}{Second-Level (Group-Level) Analysis}{equation.2.6.15}{}}
\newlabel{eq:practical_group_GLM}{{2.16}{27}{Second-Level (Group-Level) Analysis}{equation.2.6.16}{}}
\newlabel{eq:practical_group_errors}{{2.17}{27}{Second-Level (Group-Level) Analysis}{equation.2.6.17}{}}
\newlabel{group_error_covariance}{{2.18}{27}{Second-Level (Group-Level) Analysis}{equation.2.6.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.8}Solving the Second-Level GLM with Homoscedastic Errors}{28}{subsection.2.6.8}}
\newlabel{eq:group_homoscedastic_errors_covariance}{{2.19}{28}{Solving the Second-Level GLM with Homoscedastic Errors}{equation.2.6.19}{}}
\newlabel{eq:homoscedastic_parameter_estimates}{{2.20}{28}{Solving the Second-Level GLM with Homoscedastic Errors}{equation.2.6.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.9}Solving the Second-Level GLM with Hetroscedastic Errors}{28}{subsection.2.6.9}}
\newlabel{weight_matrix}{{2.21}{28}{Solving the Second-Level GLM with Hetroscedastic Errors}{equation.2.6.21}{}}
\newlabel{eq:WLS_estimates}{{2.22}{28}{Solving the Second-Level GLM with Hetroscedastic Errors}{equation.2.6.22}{}}
\newlabel{eq:WLS_t_test}{{2.23}{28}{Solving the Second-Level GLM with Hetroscedastic Errors}{equation.2.6.23}{}}
\citation{Searle2009-ku,Woolrich2004-ng,Worsley2000-rb}
\newlabel{eq:whitened_group_model}{{2.24}{29}{Solving the Second-Level GLM with Hetroscedastic Errors}{equation.2.6.24}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}The Multiple Comparisons Problem}{29}{section.2.7}}
\citation{Benjamini1995-yy}
\citation{Genovese2002-xm}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.1}Random Field Theory for Voxelwise FWE Correction}{31}{subsection.2.7.1}}
\newlabel{sec:RFT}{{2.7.1}{31}{Random Field Theory for Voxelwise FWE Correction}{subsection.2.7.1}{}}
\newlabel{eq:FWE_and_global_maximum}{{2.25}{31}{Random Field Theory for Voxelwise FWE Correction}{equation.2.7.25}{}}
\newlabel{eq:prob_EC}{{2.26}{31}{Random Field Theory for Voxelwise FWE Correction}{equation.2.7.26}{}}
\newlabel{eq:EC_high_threshold}{{2.27}{32}{Random Field Theory for Voxelwise FWE Correction}{equation.2.7.27}{}}
\newlabel{eq:EC_closed_form_approx}{{2.28}{32}{Random Field Theory for Voxelwise FWE Correction}{equation.2.7.28}{}}
\newlabel{Lamda_determinant}{{2.29}{32}{Random Field Theory for Voxelwise FWE Correction}{equation.2.7.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.2}Permutation Testing for Voxelwise FWE Correction}{32}{subsection.2.7.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Conclusion}{34}{section.2.8}}
\citation{David2013-iz,Ioannidis2014-yn}
\citation{Poldrack2017-rr,Gorgolewski2016-fk,Open_Science_Collaboration2015-gr}
\citation{Gronenschild2012-gf}
\citation{Glatard2015-ml}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Exploring the Impact of Analysis Software on Task-fMRI Results}{35}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:software}{{3}{35}{Exploring the Impact of Analysis Software on Task-fMRI Results}{chapter.3}{}}
\citation{Schonberg2012-oo}
\citation{Moran2012-cw}
\citation{Padmanabhan2011-dc}
\citation{Gorgolewski2016-nf}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Data and Analysis Methods}{36}{section.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Study Description and Data Source}{36}{subsection.3.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Data Analyses}{37}{subsection.3.1.2}}
\@writefile{toc}{\contentsline {subsubsection}{Pipeline}{37}{subsection.3.1.2}}
\citation{Maumet2016-se}
\citation{Gorgolewski2015-vs}
\citation{Smith2002-vw}
\@writefile{toc}{\contentsline {subsubsection}{Common Processing Steps}{38}{table.caption.13}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces \textbf  {Software Processing Steps.} Implementation of each of the processing steps (ds000001, ds000109, ds000120) within AFNI, FSL and SPM.\relax }}{39}{table.caption.11}}
\newlabel{tab:software_processing}{{3.1}{39}{\textbf {Software Processing Steps.} Implementation of each of the processing steps (ds000001, ds000109, ds000120) within AFNI, FSL and SPM.\relax }{table.caption.11}{}}
\citation{Andersson2007-lc}
\citation{Jenkinson2002-qf}
\@writefile{toc}{\contentsline {subsubsection}{ds000001 Analyses}{42}{table.caption.13}}
\@writefile{toc}{\contentsline {subsubsection}{AFNI Implementation}{43}{table.caption.13}}
\@writefile{toc}{\contentsline {subsubsection}{FSL Implementation}{44}{table.caption.13}}
\@writefile{toc}{\contentsline {subsubsection}{SPM Implementation}{44}{table.caption.13}}
\@writefile{toc}{\contentsline {subsubsection}{ds000109 Analyses}{44}{table.caption.13}}
\@writefile{toc}{\contentsline {subsubsection}{AFNI Implementation}{44}{table.caption.13}}
\@writefile{toc}{\contentsline {subsubsection}{FSL Implementation}{45}{table.caption.13}}
\@writefile{toc}{\contentsline {subsubsection}{SPM Implementation}{45}{table.caption.13}}
\@writefile{toc}{\contentsline {subsubsection}{ds000120 Analyses}{45}{table.caption.13}}
\@writefile{toc}{\contentsline {subsubsection}{AFNI Implementation}{45}{table.caption.13}}
\@writefile{toc}{\contentsline {subsubsection}{FSL Implementation}{46}{table.caption.13}}
\@writefile{toc}{\contentsline {subsubsection}{SPM Implementation}{46}{table.caption.13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Comparison Methods}{46}{subsection.3.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Permutation Test Methods}{48}{subsection.3.1.4}}
\citation{Winkler2016-mw}
\citation{Nichols2002-kf}
\@writefile{toc}{\contentsline {subsubsection}{AFNI Implementation}{49}{subsection.3.1.4}}
\@writefile{toc}{\contentsline {subsubsection}{FSL Implementation}{49}{subsection.3.1.4}}
\@writefile{toc}{\contentsline {subsubsection}{SPM Implementation}{49}{subsection.3.1.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.5}Scripting of Analyses and Figures}{49}{subsection.3.1.5}}
\citation{Kluyver2016-yl}
\citation{Brett2017-zb}
\citation{Walt2011-db}
\citation{McKinney2010-dv}
\citation{Hunter2007-nu}
\citation{Abraham2014-ap}
\citation{Erin_D_Foster2017-au}
\citation{Bowring2018-wp}
\citation{Nielsen2014-vl}
\citation{Bowring2018-jp}
\citation{Schonberg2012-oo}
\citation{Moran2012-cw}
\citation{Padmanabhan2011-dc}
\citation{Schonberg2012-oo}
\citation{Moran2012-cw}
\citation{Padmanabhan2011-dc}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Results}{50}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Cross-Software Variability for Parametric Inference}{50}{subsection.3.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Comparison of the thresholded statistic maps from our reanalysis with the main figures from each of the three publications. Left: For ds000001 data, thresholded $t$-statistic images contrasting the parametric modulation of pumps of reward balloons versus the parametric modulation of the control balloon; beneath, a sagittal slice taken from Fig. 3 in \citet  {Schonberg2012-oo}. Middle: For ds000109, thresholded $t$-statistic maps of the false belief versus false photo contrast; beneath, a midsagittal render from \citet  {Moran2012-cw}. Right: For ds000120, thresholded $F$-statistic images of the main effect of time contrast; beneath, a midsagittal render from Fig. 3 in \citet  {Padmanabhan2011-dc}. Note that for ds000109 and ds000120 the publication's figures are renderings onto the cortical surface while our results are slice views. While each major activation area found in the original study exists in the reanalyses, there is substantial variation between each reanalysis.\relax }}{51}{figure.caption.14}}
\newlabel{fig:SC_thresholded_maps_1}{{3.1}{51}{Comparison of the thresholded statistic maps from our reanalysis with the main figures from each of the three publications. Left: For ds000001 data, thresholded $t$-statistic images contrasting the parametric modulation of pumps of reward balloons versus the parametric modulation of the control balloon; beneath, a sagittal slice taken from Fig. 3 in \citet {Schonberg2012-oo}. Middle: For ds000109, thresholded $t$-statistic maps of the false belief versus false photo contrast; beneath, a midsagittal render from \citet {Moran2012-cw}. Right: For ds000120, thresholded $F$-statistic images of the main effect of time contrast; beneath, a midsagittal render from Fig. 3 in \citet {Padmanabhan2011-dc}. Note that for ds000109 and ds000120 the publication's figures are renderings onto the cortical surface while our results are slice views. While each major activation area found in the original study exists in the reanalyses, there is substantial variation between each reanalysis.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Comparison of the thresholded statistic maps from our reanalysis displayed as a series of axial slices. Top: ds000001's thresholded $t$-statistic maps contrasting parametric modulations of the reward balloons versus pumps of the control balloons. Middle: ds000109's thresholded $t$-statistic maps of the false belief versus false photo contrast. Bottom: ds000120's thresholded $F$-statistic maps of the main effect of time contrast. This figure complements the single slice views shown in Fig. \ref  {fig:SC_thresholded_maps_1}.\relax }}{52}{figure.caption.15}}
\newlabel{fig:SC_thresholded_maps_2}{{3.2}{52}{Comparison of the thresholded statistic maps from our reanalysis displayed as a series of axial slices. Top: ds000001's thresholded $t$-statistic maps contrasting parametric modulations of the reward balloons versus pumps of the control balloons. Middle: ds000109's thresholded $t$-statistic maps of the false belief versus false photo contrast. Bottom: ds000120's thresholded $F$-statistic maps of the main effect of time contrast. This figure complements the single slice views shown in Fig. \ref {fig:SC_thresholded_maps_1}.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Comparison of the unthresholded statistic maps from our reanalysis of the three studies within each software package. Left: ds000001's unthresholded $t$-statistic maps of the parametric modulation of pumps of reward balloons versus the parametric modulation of the control balloon contrast. Middle: ds000109's unthresholded $t$-statistic maps of the false belief versus false photo contrast. Right: ds000120's unthresholded $F$-statistic maps of the main effect of time contrast. While areas of strong activation are somewhat consistent across all three sets of reanalyses, there is substantial variation in non-extreme values.\relax }}{53}{figure.caption.16}}
\newlabel{fig:SC_unthresholded_maps}{{3.3}{53}{Comparison of the unthresholded statistic maps from our reanalysis of the three studies within each software package. Left: ds000001's unthresholded $t$-statistic maps of the parametric modulation of pumps of reward balloons versus the parametric modulation of the control balloon contrast. Middle: ds000109's unthresholded $t$-statistic maps of the false belief versus false photo contrast. Right: ds000120's unthresholded $F$-statistic maps of the main effect of time contrast. While areas of strong activation are somewhat consistent across all three sets of reanalyses, there is substantial variation in non-extreme values.\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Cross-software Bland-Altman 2D histograms comparing the unthresholded group-level $t$-statistic maps computed as part our reanalyses of the ds000001 and ds000109 studies within AFNI, FSL, and SPM. Left; Comparisons for ds000001's balloon analog risk task, $t$-statistic images contrasting the parametric modulation of pumps of the reward balloons versus parametric modulation of pumps of the control balloon. Right; Comparisons for ds000109's false belief task, $t$-statistic images contrasting the false belief versus false photo conditions. Density images show the relationship between the average $t$-statistic value (abscissa) and difference of $t$-statistic values (ordinate) at corresponding voxels in the unthresholded $t$-statistic images for each pairwise combination of software packages. While there is no particular pattern of bias, as the $t$-statistic differences are centered about zero, there is remarkable range, with differences exceeding $\pm $ 4 in all comparisons.\relax }}{54}{figure.caption.17}}
\newlabel{fig:BA_parametric}{{3.4}{54}{Cross-software Bland-Altman 2D histograms comparing the unthresholded group-level $t$-statistic maps computed as part our reanalyses of the ds000001 and ds000109 studies within AFNI, FSL, and SPM. Left; Comparisons for ds000001's balloon analog risk task, $t$-statistic images contrasting the parametric modulation of pumps of the reward balloons versus parametric modulation of pumps of the control balloon. Right; Comparisons for ds000109's false belief task, $t$-statistic images contrasting the false belief versus false photo conditions. Density images show the relationship between the average $t$-statistic value (abscissa) and difference of $t$-statistic values (ordinate) at corresponding voxels in the unthresholded $t$-statistic images for each pairwise combination of software packages. While there is no particular pattern of bias, as the $t$-statistic differences are centered about zero, there is remarkable range, with differences exceeding $\pm $ 4 in all comparisons.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Cross-software Bland-Altman 2D histogram comparing the unthresholded main effect of time $F$-statistic maps computed in AFNI and SPM for reanalyses of the ds000120 study. The differences are generally centered about zero, with a trend of large $F$-statistics for AFNI. The funnel-like pattern is a consequence of the $F$-statistic taking on only positive values.\relax }}{55}{figure.caption.18}}
\newlabel{fig:BA_parametric_120}{{3.5}{55}{Cross-software Bland-Altman 2D histogram comparing the unthresholded main effect of time $F$-statistic maps computed in AFNI and SPM for reanalyses of the ds000120 study. The differences are generally centered about zero, with a trend of large $F$-statistics for AFNI. The funnel-like pattern is a consequence of the $F$-statistic taking on only positive values.\relax }{figure.caption.18}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces \textbf  {Neurosynth Analyses.} The Neurosynth analysis terms most strongly associated (via Pearson correlation) to each software's group-level statistic map across the three studies. Non-anatomical terms are shown in bold.\relax }}{57}{table.caption.19}}
\newlabel{tab:neurosynth}{{3.2}{57}{\textbf {Neurosynth Analyses.} The Neurosynth analysis terms most strongly associated (via Pearson correlation) to each software's group-level statistic map across the three studies. Non-anatomical terms are shown in bold.\relax }{table.caption.19}{}}
\newlabel{RF1}{58}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Cross-software Bland-Altman 2D histogram comparing the unthresholded main effect of time $F$-statistic maps computed in AFNI and SPM for reanalyses of the ds000120 study. The differences are generally centered about zero, with a trend of large $F$-statistics for AFNI. The funnel-like pattern is a consequence of the $F$-statistic taking on only positive values.\relax }}{58}{figure.caption.21}}
\newlabel{fig:DICE}{{3.6}{58}{Cross-software Bland-Altman 2D histogram comparing the unthresholded main effect of time $F$-statistic maps computed in AFNI and SPM for reanalyses of the ds000120 study. The differences are generally centered about zero, with a trend of large $F$-statistics for AFNI. The funnel-like pattern is a consequence of the $F$-statistic taking on only positive values.\relax }{figure.caption.21}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces Summary of test statistics mean differences and correlations for each pair of test statistic images. Mean differences correspond to the y-axes of the Bland-Altman plots displayed in Figures \ref  {fig:BA_parametric}, \ref  {fig:BA_parametric}, \ref  {fig:BA_permutation} and \ref  {fig:BA_intrasoftware}. Each mean difference is the first item minus second; for example, AFNI versus FSL mean difference is AFNI-FSL. Correlation is the Pearson's r between the test statistic values for the pair compared. Intersoftware differences are greater than intrasoftware.\relax }}{59}{table.caption.20}}
\newlabel{tab:mean_diff_correlation}{{3.3}{59}{Summary of test statistics mean differences and correlations for each pair of test statistic images. Mean differences correspond to the y-axes of the Bland-Altman plots displayed in Figures \ref {fig:BA_parametric}, \ref {fig:BA_parametric}, \ref {fig:BA_permutation} and \ref {fig:BA_intrasoftware}. Each mean difference is the first item minus second; for example, AFNI versus FSL mean difference is AFNI-FSL. Correlation is the Pearson's r between the test statistic values for the pair compared. Intersoftware differences are greater than intrasoftware.\relax }{table.caption.20}{}}
\newlabel{RF2}{60}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Euler characteristic (EC) plots for ds000001 and ds000109. On top, comparisons of the Euler characteristic computed for each software's $t$-statistic map from our reanalyses using a range of $t$-value thresholds between -6 and 6. Below, comparisons of the ECs calculated using the same thresholds on the corresponding $t$-statistic images for permutation inference within each package. For each $t$-value the EC summarises the topology of the thresholded image, and the curves provide a signature of the structure of the entire image. For extreme thresholds the EC approximates the number of clusters, allowing a simple interpretation of the curves: For example, for ds000001 parametric analyses, FSL clearly has the fewest clusters for positive thresholds.\relax }}{60}{figure.caption.22}}
\newlabel{fig:ECs}{{3.7}{60}{Euler characteristic (EC) plots for ds000001 and ds000109. On top, comparisons of the Euler characteristic computed for each software's $t$-statistic map from our reanalyses using a range of $t$-value thresholds between -6 and 6. Below, comparisons of the ECs calculated using the same thresholds on the corresponding $t$-statistic images for permutation inference within each package. For each $t$-value the EC summarises the topology of the thresholded image, and the curves provide a signature of the structure of the entire image. For extreme thresholds the EC approximates the number of clusters, allowing a simple interpretation of the curves: For example, for ds000001 parametric analyses, FSL clearly has the fewest clusters for positive thresholds.\relax }{figure.caption.22}{}}
\newlabel{RF3}{61}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Cluster count plots for ds000001 and ds000109. On top, comparisons of the number of cluster found in each software's $t$-statistic map from our reanalyses using a range of $t$-value thresholds between -6 and 6. Below, comparisons of the cluster counts calculated using the same thresholds on the corresponding $t$-statistic images for permutation inference within each package.\relax }}{61}{figure.caption.23}}
\newlabel{fig:cluster_count}{{3.8}{61}{Cluster count plots for ds000001 and ds000109. On top, comparisons of the number of cluster found in each software's $t$-statistic map from our reanalyses using a range of $t$-value thresholds between -6 and 6. Below, comparisons of the cluster counts calculated using the same thresholds on the corresponding $t$-statistic images for permutation inference within each package.\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Cross-Software Variability for Nonparametric Inference}{62}{subsection.3.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Intra-Software Variability, Parametric vs Nonparametric}{63}{subsection.3.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Cross-software Bland-Altman 2D histograms for the ds000001 and ds000109 studies comparing the unthresholded group-level $t$-statistic maps computed using permutation inference methods within AFNI, FSL, and SPM. Similar to the results obtained using parametric inferences in Figure \ref  {fig:BA_parametric}, all of the densities indicate large differences in the size of activations determined within each package.\relax }}{64}{figure.caption.24}}
\newlabel{fig:BA_permutation}{{3.9}{64}{Cross-software Bland-Altman 2D histograms for the ds000001 and ds000109 studies comparing the unthresholded group-level $t$-statistic maps computed using permutation inference methods within AFNI, FSL, and SPM. Similar to the results obtained using parametric inferences in Figure \ref {fig:BA_parametric}, all of the densities indicate large differences in the size of activations determined within each package.\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Intrasoftware Bland-Altman 2D histograms for the ds000001 and ds000109 studies comparing the unthresholded group-level $t$-statistic maps computed for parametric and nonparametric inference methods in AFNI, FSL and SPM. Each comparison here uses the same preprocessed data, varying only the second level statistical model. SPM's parametric and nonparametric both use the same (unweighted) one-sample $t$-test, and thus show no differences. AFNI and FSL's parametric models use iterative estimation of between subject variance and weighted least squares and thus show some differences, but still smaller than between-software comparisons\relax }}{65}{figure.caption.25}}
\newlabel{fig:BA_intrasoftware}{{3.10}{65}{Intrasoftware Bland-Altman 2D histograms for the ds000001 and ds000109 studies comparing the unthresholded group-level $t$-statistic maps computed for parametric and nonparametric inference methods in AFNI, FSL and SPM. Each comparison here uses the same preprocessed data, varying only the second level statistical model. SPM's parametric and nonparametric both use the same (unweighted) one-sample $t$-test, and thus show no differences. AFNI and FSL's parametric models use iterative estimation of between subject variance and weighted least squares and thus show some differences, but still smaller than between-software comparisons\relax }{figure.caption.25}{}}
\citation{Chumbley2009-ce}
\citation{Eklund2016-ak}
\citation{Eklund2016-ak}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Discussion}{66}{section.3.3}}
\citation{Olszowy2019-cz}
\citation{Strother2002-sz}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Limitations}{68}{subsection.3.3.1}}
\citation{Alfaro-Almagro2018-ip}
\citation{Ellis2019-zi}
\citation{Poldrack2017-rr}
\citation{Yeung2018-kr}
\citation{Mueller2017-pn}
\citation{Eklund2016-ak,Woo2014-ji}
\citation{Chen2018-am}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Conclusion}{70}{section.3.4}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Spatial Confidence Sets for Raw Effect Size Images}{72}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:BOLD}{{4}{72}{Spatial Confidence Sets for Raw Effect Size Images}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{72}{section.4.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Theory}{72}{section.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Overview}{72}{subsection.4.2.1}}
\newlabel{sec:Overview}{{4.2.1}{72}{Overview}{subsection.4.2.1}{}}
\newlabel{eq:CI_GLM}{{4.1}{72}{Overview}{equation.4.2.1}{}}
\citation{Poldrack2011-bw}
\newlabel{eq:CI_excursion}{{4.2}{73}{Overview}{equation.4.2.2}{}}
\newlabel{thm:SSS_result}{{1}{73}{}{theorem.1}{}}
\newlabel{fig:one_d_intuiton}{{\caption@xref {fig:one_d_intuiton}{ on input line 44}}{74}{Overview}{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces A demonstration of how the CSs are computed for a realization of the GLM $\bm  {Y}(\bm  {s}) = \bm  {X}\bm  {\beta }(\bm  {s}) + \bm  {\epsilon }(\bm  {s})$ in 1 dimension, for each location $\bm  {s}$. The yellow voxels $\Ahatc  $ are obtained by thresholding the observed group contrast map at threshold $c$; this is the best guess of $\Ac  $, the set of voxels whose true, noise-free raw effect surpasses $c$. The red upper CS $\Ahatcp  $ and blue lower CS $\Ahatcm  $ are computed by thresholding the signal at $c + k\tmspace  +\thinmuskip {.1667em} \mathaccentV {hat}05E{\sigma }(\bm  {s}) v_{w}$ and $c - k\tmspace  +\thinmuskip {.1667em} \mathaccentV {hat}05E{\sigma }(\bm  {s}) v_{w}$, respectively. We have $(1-\alpha )100\%$ confidence that $\Ahatcp  \subset \Ac  \subset \Ahatcm  $, i.e.\nobreakspace  {}that $\Ahatcp  $ (red) is completely within the true $\Ac  $, and $\Ac  $ is completely within $\Ahatcp  $ (blue). We find the critical value $k$ from the $(1-\alpha )100$ percentile of the maximum distribution of the absolute error process over the estimated boundary $\dAhatc  $ (green \unhbox \voidb@x \hbox {\fontencoding  {U}\fontfamily  {wasy}\selectfont  \char 32}'s) using the Wild $t$-Bootstrap; $\mathaccentV {hat}05E\sigma $ is the estimated standard deviation and $v_w$ is the normalised contrast variance.\relax }}{74}{figure.caption.26}}
\newlabel{eq:probability_statement}{{4.3}{74}{Overview}{equation.4.2.3}{}}
\citation{Chernozhukov2013-wz}
\citation{Davidson2008-qh}
\citation{Telschow2019-lg}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}The Wild Bootstrap Method for Computation of \textit  {k}}{75}{subsection.4.2.2}}
\newlabel{sec:wild_bootstrap}{{4.2.2}{75}{The Wild Bootstrap Method for Computation of \textit {k}}{subsection.4.2.2}{}}
\newlabel{eq:SSS_standardized_residuals}{{4.4}{75}{The Wild Bootstrap Method for Computation of \textit {k}}{equation.4.2.4}{}}
\newlabel{eq:SSS_residual_field}{{4.5}{75}{The Wild Bootstrap Method for Computation of \textit {k}}{equation.4.2.5}{}}
\newlabel{eq:wild_bootstrap_G}{{4.6}{76}{The Wild Bootstrap Method for Computation of \textit {k}}{equation.4.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Approximating the Boundary on a Discrete Lattice}{76}{subsection.4.2.3}}
\newlabel{sec:boundary_discrete_lattice}{{4.2.3}{76}{Approximating the Boundary on a Discrete Lattice}{subsection.4.2.3}{}}
\newlabel{eq:interpolation_weights}{{4.7}{77}{Approximating the Boundary on a Discrete Lattice}{equation.4.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Assessment of Continuous Coverage on a Discrete Lattice}{77}{subsection.4.2.4}}
\newlabel{sec:coverage_assessment}{{4.2.4}{77}{Assessment of Continuous Coverage on a Discrete Lattice}{subsection.4.2.4}{}}
\newlabel{fig:lowres}{{4.2(a)}{78}{Subfigure 4 4.2(a)}{subfigure.4.2.1}{}}
\newlabel{sub@fig:lowres}{{(a)}{78}{Subfigure 4 4.2(a)\relax }{subfigure.4.2.1}{}}
\newlabel{fig:highres}{{4.2(b)}{78}{Subfigure 4 4.2(b)}{subfigure.4.2.2}{}}
\newlabel{sub@fig:highres}{{(b)}{78}{Subfigure 4 4.2(b)\relax }{subfigure.4.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Demonstrating the resolution issue for testing the subset condition $\Ahatcp  \subset \Ac  \ \subset \Ahatcm  $.   \textbf  {Figure \ref  {fig:lowres}:} Here $\Ac  $ is comprised of the right half of the image (all green and yellow pixels), and $\Ahatcp  $ is shown as yellow pixels. It appears that $\Ahatcp  \subset \Ac  $.   \textbf  {Figure \ref  {fig:highres}:} The same configuration as \textbf  {Fig. \ref  {fig:lowres}} at double the resolution. Here, we have enough detail to see that $\Ahatcp  $ has crossed the boundary $\dAc  $ (yellow seeping into blue), and the subset condition $\Ahatcp  \subset \Ac  $ has been violated.\relax }}{78}{figure.caption.27}}
\newlabel{fig:highres_lowres}{{4.2}{78}{Demonstrating the resolution issue for testing the subset condition $\Ahatcp \subset \Ac \ \subset \Ahatcm $. \\ \textbf {Figure \ref {fig:lowres}:} Here $\Ac $ is comprised of the right half of the image (all green and yellow pixels), and $\Ahatcp $ is shown as yellow pixels. It appears that $\Ahatcp \subset \Ac $. \\ \textbf {Figure \ref {fig:highres}:} The same configuration as \textbf {Fig. \ref {fig:lowres}} at double the resolution. Here, we have enough detail to see that $\Ahatcp $ has crossed the boundary $\dAc $ (yellow seeping into blue), and the subset condition $\Ahatcp \subset \Ac $ has been violated.\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Low Resolution Simulation}}}{78}{figure.caption.27}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {High Resolution Simulation}}}{78}{figure.caption.27}}
\newlabel{eq:interpolation_in_action}{{4.8}{79}{Assessment of Continuous Coverage on a Discrete Lattice}{equation.4.2.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Method}{79}{section.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Simulations}{79}{subsection.4.3.1}}
\newlabel{sec:simulations}{{4.3.1}{79}{Simulations}{subsection.4.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}2D Simulations}{80}{subsection.4.3.2}}
\newlabel{sec:2D_simulations}{{4.3.2}{80}{2D Simulations}{subsection.4.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}3D Simulations}{80}{subsection.4.3.3}}
\newlabel{sec:3D_simulations}{{4.3.3}{80}{3D Simulations}{subsection.4.3.3}{}}
\newlabel{fig:2D_signals}{{\caption@xref {fig:2D_signals}{ on input line 153}}{81}{2D Simulations}{figure.caption.28}{}}
\newlabel{fig:linear_ramp}{{4.3(a)}{81}{Subfigure 4 4.3(a)}{subfigure.4.3.1}{}}
\newlabel{sub@fig:linear_ramp}{{(a)}{81}{Subfigure 4 4.3(a)\relax }{subfigure.4.3.1}{}}
\newlabel{fig:circle}{{4.3(b)}{81}{Subfigure 4 4.3(b)}{subfigure.4.3.2}{}}
\newlabel{sub@fig:circle}{{(b)}{81}{Subfigure 4 4.3(b)\relax }{subfigure.4.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Linear ramp and circular signals $\mu (\bm  {s})$.   \textbf  {Figure \ref  {fig:linear_ramp}:} \textbf  {Signal 1.} A linear ramp signal that increases from magnitude of 1 to 3 in the x-direction.   \textbf  {Figure \ref  {fig:circle}:} \textbf  {Signal 2.} A circular signal with magnitude of 3 and radius of 30, centred within the region and convolved with a 3 voxel FWHM Gaussian kernel.\relax }}{81}{figure.caption.28}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {\textbf {Signal 1.}}}}{81}{figure.caption.28}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {\textbf {Signal 2.}}}}{81}{figure.caption.28}}
\newlabel{fig:2D_variances}{{\caption@xref {fig:2D_variances}{ on input line 161}}{81}{2D Simulations}{figure.caption.29}{}}
\newlabel{fig:stationary_variance}{{4.4(a)}{81}{Subfigure 4 4.4(a)}{subfigure.4.4.1}{}}
\newlabel{sub@fig:stationary_variance}{{(a)}{81}{Subfigure 4 4.4(a)\relax }{subfigure.4.4.1}{}}
\newlabel{fig:non_stationary_variance}{{4.4(b)}{81}{Subfigure 4 4.4(b)}{subfigure.4.4.2}{}}
\newlabel{sub@fig:non_stationary_variance}{{(b)}{81}{Subfigure 4 4.4(b)\relax }{subfigure.4.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Stationary and non-stationary standard deviation fields of the noise $\epsilon _{i}(\bm  {s})$.   \textbf  {Figure \ref  {fig:stationary_variance}:} \textbf  {Standard Deviation 1.} Stationary variance of 1 across the region.   \textbf  {Figure \ref  {fig:non_stationary_variance}:} \textbf  {Standard Deviation 2.} Non-stationary (linear ramp) standard deviation field increasing from $\sqrt  {0.5}$ to $\sqrt  {1.5}$ in the y-direction.\relax }}{81}{figure.caption.29}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {\textbf {Signal 1.}}}}{81}{figure.caption.29}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {\textbf {Signal 2.}}}}{81}{figure.caption.29}}
\citation{Miller2016-hd}
\citation{Alfaro-Almagro2018-ip}
\citation{Hariri2002-ns}
\citation{Barch2013-kk}
\citation{Glasser2013-qc}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Application to Human Connectome Project Data}{82}{subsection.4.3.4}}
\newlabel{sec:HCP_methods}{{4.3.4}{82}{Application to Human Connectome Project Data}{subsection.4.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces The four 3D signal types $\mu (\bm  {s})$, from top-to-bottom: small sphere, large sphere, multiple spheres, and the UK Biobank full mean image. Note that the colormap limits for the first three signal types are from 0 to 3, while the colormap limits for the UK Biobank mean image is from -0.4 to 0.5.\relax }}{83}{table.caption.30}}
\newlabel{tbl:3D_figures}{{4.5}{83}{The four 3D signal types $\mu (\bm {s})$, from top-to-bottom: small sphere, large sphere, multiple spheres, and the UK Biobank full mean image. Note that the colormap limits for the first three signal types are from 0 to 3, while the colormap limits for the UK Biobank mean image is from -0.4 to 0.5.\relax }{table.caption.30}{}}
\citation{Glasser2013-qc}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Results}{85}{section.4.4}}
\newlabel{sec:Results}{{4.4}{85}{Results}{section.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Methodological Comparisons}{85}{subsection.4.4.1}}
\newlabel{sec:method_comparisons}{{4.4.1}{85}{Methodological Comparisons}{subsection.4.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Coverage results for the 2D circular signal simulation with homogeneous Gaussian noise (\textbf  {Signal 2.} (\textbf  {Standard deviation 1.}) in Fig. \ref  {fig:circle} (Fig. \ref  {fig:stationary_variance}). Empirical coverage results are presented for implementations of the CS method with and without the Wild $t$-Bootstrap we propose in Section \ref  {sec:wild_bootstrap} and the interpolation schema for assessing simulations results we propose in Section \ref  {sec:coverage_assessment}. All empirical coverage results for simulations using the \textit  {SSS} assessment method are close to 100\%, suggesting that this assessment substantially biases the results upwards. Using our proposed assessment method, while both the Wild $t$-Bootstrap and Gaussian Wild bootstrap converge to the nominal level, the Wild $t$-Bootstrap performed better for small sample sizes.\relax }}{86}{figure.caption.31}}
\newlabel{fig:2D_method_comparisons}{{4.6}{86}{Coverage results for the 2D circular signal simulation with homogeneous Gaussian noise (\textbf {Signal 2.} (\textbf {Standard deviation 1.}) in Fig. \ref {fig:circle} (Fig. \ref {fig:stationary_variance}). Empirical coverage results are presented for implementations of the CS method with and without the Wild $t$-Bootstrap we propose in Section \ref {sec:wild_bootstrap} and the interpolation schema for assessing simulations results we propose in Section \ref {sec:coverage_assessment}. All empirical coverage results for simulations using the \textit {SSS} assessment method are close to 100\%, suggesting that this assessment substantially biases the results upwards. Using our proposed assessment method, while both the Wild $t$-Bootstrap and Gaussian Wild bootstrap converge to the nominal level, the Wild $t$-Bootstrap performed better for small sample sizes.\relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}2D Simulations}{86}{subsection.4.4.2}}
\newlabel{sec:2D_simulation_results}{{4.4.2}{86}{2D Simulations}{subsection.4.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Coverage results for the 3D large spherical signal (\textbf  {Signal 2.} in Fig. \ref  {tbl:3D_figures}(b)) simulation with homogeneous Gaussian noise. Empirical coverage results are presented for implementations of the CS method with and without the Wild $t$-Bootstrap we propose in Section \ref  {sec:wild_bootstrap}, and the interpolation schema for assessing simulations results we propose in Section \ref  {sec:coverage_assessment}. Once again, all simulations using the \textit  {SSS} assessment method quickly converge to close to 100\%. Using our proposed assessment method, the Gaussian Wild bootstrap had severe under-coverage for small sample sizes, while the Wild $t$-Bootstrap results hover slightly above the nominal level for all sample sizes.\relax }}{87}{figure.caption.32}}
\newlabel{fig:3D_method_comparisons}{{4.7}{87}{Coverage results for the 3D large spherical signal (\textbf {Signal 2.} in Fig. \ref {tbl:3D_figures}(b)) simulation with homogeneous Gaussian noise. Empirical coverage results are presented for implementations of the CS method with and without the Wild $t$-Bootstrap we propose in Section \ref {sec:wild_bootstrap}, and the interpolation schema for assessing simulations results we propose in Section \ref {sec:coverage_assessment}. Once again, all simulations using the \textit {SSS} assessment method quickly converge to close to 100\%. Using our proposed assessment method, the Gaussian Wild bootstrap had severe under-coverage for small sample sizes, while the Wild $t$-Bootstrap results hover slightly above the nominal level for all sample sizes.\relax }{figure.caption.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}3D Simulations}{88}{subsection.4.4.3}}
\newlabel{sec:3D_simulation_results}{{4.4.3}{88}{3D Simulations}{subsection.4.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Coverage results for \textbf  {Signal 1.}, the 2D linear ramp signal. While the true boundary coverage results (dashed curves) fall under the nominal level, results for the estimated boundary method (solid curves) that must be applied to real data remain above the nominal level. Performance of the method improved for larger confidence levels, and in particular, the estimated boundary results for a 95\% confidence level seen in the right plot hover slightly above nominal coverage for all sample sizes.\relax }}{89}{figure.caption.33}}
\newlabel{fig:2D_sig_1_results}{{4.8}{89}{Coverage results for \textbf {Signal 1.}, the 2D linear ramp signal. While the true boundary coverage results (dashed curves) fall under the nominal level, results for the estimated boundary method (solid curves) that must be applied to real data remain above the nominal level. Performance of the method improved for larger confidence levels, and in particular, the estimated boundary results for a 95\% confidence level seen in the right plot hover slightly above nominal coverage for all sample sizes.\relax }{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Coverage results for \textbf  {Signal 2.}, the 2D circular signal. Coverage performance was close to nominal level in all simulations. The method was robust as to whether the subject-level noise had homogeneous (red curves) or heterogeneous variance (blue curves), or as to whether the estimated boundary (dashed curves) or true boundary (solid curves) method was used; in all plots, all of the curves lie practically on top of each other.\relax }}{89}{figure.caption.34}}
\newlabel{fig:2D_sig_2_results}{{4.9}{89}{Coverage results for \textbf {Signal 2.}, the 2D circular signal. Coverage performance was close to nominal level in all simulations. The method was robust as to whether the subject-level noise had homogeneous (red curves) or heterogeneous variance (blue curves), or as to whether the estimated boundary (dashed curves) or true boundary (solid curves) method was used; in all plots, all of the curves lie practically on top of each other.\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Coverage results for \textbf  {Signal 1.}, the 3D small spherical signal. For all confidence levels, coverage remained above the nominal level in all simulations, and for a 95\% confidence level (right plot), coverage hovered slightly above the nominal level for all sample sizes. The method was robust as to whether the subject-level noise had homogeneous (red curves) or heterogeneous variance (blue curves), or as to whether the estimated boundary (dashed curves) or true boundary (solid curves) method was used.\relax }}{90}{figure.caption.35}}
\newlabel{fig:3D_sig_1_results}{{4.10}{90}{Coverage results for \textbf {Signal 1.}, the 3D small spherical signal. For all confidence levels, coverage remained above the nominal level in all simulations, and for a 95\% confidence level (right plot), coverage hovered slightly above the nominal level for all sample sizes. The method was robust as to whether the subject-level noise had homogeneous (red curves) or heterogeneous variance (blue curves), or as to whether the estimated boundary (dashed curves) or true boundary (solid curves) method was used.\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Coverage results for \textbf  {Signal 2.}, the large 3D spherical signal. Coverage results here were very similar to the results for the small spherical signal shown in Fig. \ref  {fig:3D_sig_1_results}, suggesting that the method is robust to changes in boundary length.\relax }}{91}{figure.caption.36}}
\newlabel{fig:3D_sig_2_results}{{4.11}{91}{Coverage results for \textbf {Signal 2.}, the large 3D spherical signal. Coverage results here were very similar to the results for the small spherical signal shown in Fig. \ref {fig:3D_sig_1_results}, suggesting that the method is robust to changes in boundary length.\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces Coverage results for \textbf  {Signal 3.}, the multiple spheres signal. Once again, for all confidence levels, coverage remained above the nominal level in all simulations. Here, the true boundary method (dashed curves) performed slightly better than the estimated boundary method (solid curves) in small sample sizes, although the choice of boundary made less of a difference for a higher confidence level. For a 95\% confidence level (right plot), all results hover slightly above nominal coverage for all sample sizes.\relax }}{91}{figure.caption.37}}
\newlabel{fig:3D_sig_3_results}{{4.12}{91}{Coverage results for \textbf {Signal 3.}, the multiple spheres signal. Once again, for all confidence levels, coverage remained above the nominal level in all simulations. Here, the true boundary method (dashed curves) performed slightly better than the estimated boundary method (solid curves) in small sample sizes, although the choice of boundary made less of a difference for a higher confidence level. For a 95\% confidence level (right plot), all results hover slightly above nominal coverage for all sample sizes.\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces  Coverage results for \textbf  {Signal 4.}, the UK Biobank full mean signal, where the full standard deviation image was used as the standard deviation of the subject-level noise fields. Coverage results here were similar to the results for the multiple spheres signal shown in Fig. \ref  {fig:3D_sig_3_results}: In small sample sizes, coverage was slightly improved for the true boundary method (dashed curves) compared to the estimated boundary method (solid curves), however, for a 95\% confidence level (right plots), all results hover slightly above nominal coverage for all sample sizes.\relax }}{92}{figure.caption.38}}
\newlabel{fig:3D_sig_4_results}{{4.13}{92}{Coverage results for \textbf {Signal 4.}, the UK Biobank full mean signal, where the full standard deviation image was used as the standard deviation of the subject-level noise fields. Coverage results here were similar to the results for the multiple spheres signal shown in Fig. \ref {fig:3D_sig_3_results}: In small sample sizes, coverage was slightly improved for the true boundary method (dashed curves) compared to the estimated boundary method (solid curves), however, for a 95\% confidence level (right plots), all results hover slightly above nominal coverage for all sample sizes.\relax }{figure.caption.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Human Connectome Project}{93}{subsection.4.4.4}}
\newlabel{HCP_results}{{4.4.4}{93}{Human Connectome Project}{subsection.4.4.4}{}}
\citation{Chen2017-sb}
\citation{Engel2013-nq}
\citation{Smith2013-ul}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Discussion}{94}{section.4.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Spatial Inference on \%BOLD Raw Effect Size}{94}{subsection.4.5.1}}
\newlabel{sec:discussion_inference}{{4.5.1}{94}{Spatial Inference on \%BOLD Raw Effect Size}{subsection.4.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces Slice views of the Confidence Sets for 80 subjects data from the HCP working memory task for $c = 1.0\%, 1.5\%$ and $2.0\%$ BOLD change thresholds. The upper CS $\Ahatcp  $ is displayed in red, and the lower CS $\Ahatcm  $ displayed in blue. In yellow is the point estimate set $\Ahatc  $, the best guess from the data of voxels that surpassed the BOLD change threshold. The red upper CS has localized regions in the frontal gyrus, frontal pole, anterior insula, supramarginal gyrus and cerebellum for which we can assert with $95\%$ confidence that there has been (at least) a $1.0\%$ BOLD change raw effect.\relax }}{95}{table.caption.39}}
\newlabel{tbl:HCP_results_one}{{4.14}{95}{Slice views of the Confidence Sets for 80 subjects data from the HCP working memory task for $c = 1.0\%, 1.5\%$ and $2.0\%$ BOLD change thresholds. The upper CS $\Ahatcp $ is displayed in red, and the lower CS $\Ahatcm $ displayed in blue. In yellow is the point estimate set $\Ahatc $, the best guess from the data of voxels that surpassed the BOLD change threshold. The red upper CS has localized regions in the frontal gyrus, frontal pole, anterior insula, supramarginal gyrus and cerebellum for which we can assert with $95\%$ confidence that there has been (at least) a $1.0\%$ BOLD change raw effect.\relax }{table.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces Further slice views of the Confidence Sets. Here, we see that the red upper CS has also localized regions in the anterior cingulate, superior front gyrus, supramarginal gyrus, and precuneous for which we can assert with $95\%$ confidence that there has been (at least) a $1.0\%$ BOLD change raw effect.\relax }}{96}{table.caption.40}}
\newlabel{tbl:HCP_results_two}{{4.15}{96}{Further slice views of the Confidence Sets. Here, we see that the red upper CS has also localized regions in the anterior cingulate, superior front gyrus, supramarginal gyrus, and precuneous for which we can assert with $95\%$ confidence that there has been (at least) a $1.0\%$ BOLD change raw effect.\relax }{table.caption.40}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Analysis of HCP data and Simulation Results}{97}{subsection.4.5.2}}
\newlabel{sec:discussion_HCP_simultations}{{4.5.2}{97}{Analysis of HCP data and Simulation Results}{subsection.4.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Methodological Innovations}{98}{subsection.4.5.3}}
\newlabel{sec:discussion_innovations}{{4.5.3}{98}{Methodological Innovations}{subsection.4.5.3}{}}
\citation{UIudag2009-nm}
\citation{Winkler2016-mw}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.4}Limitations}{99}{subsection.4.5.4}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Conclusion}{99}{section.4.6}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Spatial Confidence Sets for Cohen's d Effect Size Images}{100}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:cohen}{{5}{100}{Spatial Confidence Sets for Cohen's d Effect Size Images}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Theory}{100}{section.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}From \%BOLD to Cohen's d}{100}{subsection.5.1.1}}
\newlabel{sec:BOLD_to_cohen}{{5.1.1}{100}{From \%BOLD to Cohen's d}{subsection.5.1.1}{}}
\newlabel{eq:Cohen_GLM}{{5.1}{100}{From \%BOLD to Cohen's d}{equation.5.1.1}{}}
\newlabel{eq:cohens_d}{{5.2}{100}{From \%BOLD to Cohen's d}{equation.5.1.2}{}}
\newlabel{eq:Ac}{{5.3}{100}{From \%BOLD to Cohen's d}{equation.5.1.3}{}}
\newlabel{eq:mu_excursion}{{5.4}{101}{From \%BOLD to Cohen's d}{equation.5.1.4}{}}
\newlabel{eq:mu_CSs}{{5.5}{101}{From \%BOLD to Cohen's d}{equation.5.1.5}{}}
\newlabel{fig:linear_ramp_true}{{5.1(a)}{102}{Subfigure 5 5.1(a)}{subfigure.5.1.1}{}}
\newlabel{sub@fig:linear_ramp_true}{{(a)}{102}{Subfigure 5 5.1(a)\relax }{subfigure.5.1.1}{}}
\newlabel{fig:linear_ramp_sample_mean}{{5.1(b)}{102}{Subfigure 5 5.1(b)}{subfigure.5.1.2}{}}
\newlabel{sub@fig:linear_ramp_sample_mean}{{(b)}{102}{Subfigure 5 5.1(b)\relax }{subfigure.5.1.2}{}}
\newlabel{fig:linear_ramp_cohen_d}{{5.1(c)}{102}{Subfigure 5 5.1(c)}{subfigure.5.1.3}{}}
\newlabel{sub@fig:linear_ramp_cohen_d}{{(c)}{102}{Subfigure 5 5.1(c)\relax }{subfigure.5.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Visualizing the differences between the sample mean and sample Cohen's $d$ field from the 2D simulation. While the sample mean image appears to be uniformly smooth across the region, the sample Cohen's $d$ field becomes rougher from left-to-right.\relax }}{102}{figure.caption.41}}
\newlabel{fig:visual_example}{{5.1}{102}{Visualizing the differences between the sample mean and sample Cohen's $d$ field from the 2D simulation. While the sample mean image appears to be uniformly smooth across the region, the sample Cohen's $d$ field becomes rougher from left-to-right.\relax }{figure.caption.41}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {The true underlying signal $\mu (\bm {s}) = d(\bm {s})$, a linear ramp increasing from 0 to 10 in the $x$-direction.}}}{102}{figure.caption.41}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {The sample mean field $\mathaccentV{bar}016{X}(\bm {s})$}}}{102}{figure.caption.41}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {The sample Cohen's $d$ field $\mathaccentV{hat}05E{d}(\bm {s}) = \frac {\mathaccentV{bar}016{X}(\bm {s})}{\mathaccentV{hat}05E{\sigma }(\bm {s})}$}}}{102}{figure.caption.41}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Limiting Properties of the Cohen's d Estimator}{103}{subsection.5.1.2}}
\newlabel{sec:gauss_sig_plus_noise}{{5.1.2}{103}{Limiting Properties of the Cohen's d Estimator}{subsection.5.1.2}{}}
\newlabel{eq:sig_plus_noise}{{5.6}{103}{Limiting Properties of the Cohen's d Estimator}{equation.5.1.6}{}}
\newlabel{eq:unbiased_estimators}{{5.7}{103}{Limiting Properties of the Cohen's d Estimator}{equation.5.1.7}{}}
\newlabel{eq:joint_limiting_distribution}{{5.8}{103}{Limiting Properties of the Cohen's d Estimator}{equation.5.1.8}{}}
\newlabel{eq:covariance_matrix}{{5.9}{103}{Limiting Properties of the Cohen's d Estimator}{equation.5.1.9}{}}
\newlabel{eq:cohen_limiting_distribution}{{5.10}{103}{Limiting Properties of the Cohen's d Estimator}{equation.5.1.10}{}}
\newlabel{eq:covariance_of_errors}{{5.11}{104}{Limiting Properties of the Cohen's d Estimator}{equation.5.1.11}{}}
\newlabel{eq:16}{{5.12}{104}{Limiting Properties of the Cohen's d Estimator}{equation.5.1.12}{}}
\newlabel{eq:17}{{5.13}{104}{Limiting Properties of the Cohen's d Estimator}{equation.5.1.13}{}}
\newlabel{eq:joint_limiting_distribution_again}{{5.14}{104}{Limiting Properties of the Cohen's d Estimator}{equation.5.1.14}{}}
\newlabel{eq:cohen_limiting_distribtuion_covariance}{{5.15}{104}{Limiting Properties of the Cohen's d Estimator}{equation.5.1.15}{}}
\newlabel{eq:cohen_limiting_covariance_matrix}{{5.16}{104}{Limiting Properties of the Cohen's d Estimator}{equation.5.1.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Spatial Confidence Sets for Cohen's d Effect Size Images}{104}{subsection.5.1.3}}
\newlabel{sec:confidence_sets_for_cohens_d}{{5.1.3}{104}{Spatial Confidence Sets for Cohen's d Effect Size Images}{subsection.5.1.3}{}}
\newlabel{eq:mu_CSs_again}{{5.17}{105}{Spatial Confidence Sets for Cohen's d Effect Size Images}{equation.5.1.17}{}}
\newlabel{eq:percent_BOLD_normalised error_field}{{5.18}{105}{Spatial Confidence Sets for Cohen's d Effect Size Images}{equation.5.1.18}{}}
\newlabel{eq:Cohens_d_normalised error_field}{{5.19}{105}{Spatial Confidence Sets for Cohen's d Effect Size Images}{equation.5.1.19}{}}
\newlabel{eq:limiting_covariance_structure}{{5.20}{105}{Spatial Confidence Sets for Cohen's d Effect Size Images}{equation.5.1.20}{}}
\newlabel{eq:cohens_CSs}{{5.21}{105}{Spatial Confidence Sets for Cohen's d Effect Size Images}{equation.5.1.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.4}Modified Residuals for the Cohen's d Wild t-bootstrap}{106}{subsection.5.1.4}}
\newlabel{sec:Wild_t_bootstrap}{{5.1.4}{106}{Modified Residuals for the Cohen's d Wild t-bootstrap}{subsection.5.1.4}{}}
\newlabel{Result_1}{{5.22}{106}{Modified Residuals for the Cohen's d Wild t-bootstrap}{equation.5.1.22}{}}
\newlabel{eq:standardized_residuals}{{5.23}{106}{Modified Residuals for the Cohen's d Wild t-bootstrap}{equation.5.1.23}{}}
\newlabel{eq:wild_bootstrap_approximating_field}{{5.24}{106}{Modified Residuals for the Cohen's d Wild t-bootstrap}{equation.5.1.24}{}}
\newlabel{eq:approximate_limiting_covariance_structure}{{5.25}{106}{Modified Residuals for the Cohen's d Wild t-bootstrap}{equation.5.1.25}{}}
\newlabel{eq:wrong_covariance}{{5.26}{107}{Modified Residuals for the Cohen's d Wild t-bootstrap}{equation.5.1.26}{}}
\newlabel{eq:single_subject_residuals}{{5.27}{107}{Modified Residuals for the Cohen's d Wild t-bootstrap}{equation.5.1.27}{}}
\newlabel{eq:approximating_residuals}{{5.28}{107}{Modified Residuals for the Cohen's d Wild t-bootstrap}{equation.5.1.28}{}}
\newlabel{eq:cohens_d_residuals}{{5.29}{107}{Modified Residuals for the Cohen's d Wild t-bootstrap}{equation.5.1.29}{}}
\newlabel{eq:cohen_d_wild_bootstrap_approximating_field}{{5.30}{107}{Modified Residuals for the Cohen's d Wild t-bootstrap}{equation.5.1.30}{}}
\newlabel{eq:sd_of_approximate_residuals}{{5.31}{108}{Modified Residuals for the Cohen's d Wild t-bootstrap}{equation.5.1.31}{}}
\newlabel{eq:alternate_cohens_d_residuals}{{5.32}{108}{Modified Residuals for the Cohen's d Wild t-bootstrap}{equation.5.1.32}{}}
\newlabel{eq:alternate_cohens_CSs}{{5.33}{108}{Modified Residuals for the Cohen's d Wild t-bootstrap}{equation.5.1.33}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.5}Finite Properties of the Cohen's d Estimator and a Variance-stabilizing Transformation}{108}{subsection.5.1.5}}
\newlabel{sec:normalizing_cohens_d}{{5.1.5}{108}{Finite Properties of the Cohen's d Estimator and a Variance-stabilizing Transformation}{subsection.5.1.5}{}}
\newlabel{eq:cohens_d_estimator}{{5.34}{108}{Finite Properties of the Cohen's d Estimator and a Variance-stabilizing Transformation}{equation.5.1.34}{}}
\citation{Laubscher1960-px}
\citation{Laubscher1960-px}
\newlabel{thm:Theorem_1}{{1}{109}{}{cohen_theorem.1}{}}
\newlabel{eq:Cn}{{5.36}{109}{Finite Properties of the Cohen's d Estimator and a Variance-stabilizing Transformation}{equation.5.1.36}{}}
\newlabel{eq:cohen_d_expectation}{{5.37}{110}{Finite Properties of the Cohen's d Estimator and a Variance-stabilizing Transformation}{equation.5.1.37}{}}
\newlabel{eq:cohen_d_variance}{{5.38}{110}{Finite Properties of the Cohen's d Estimator and a Variance-stabilizing Transformation}{equation.5.1.38}{}}
\newlabel{eq:Cn_approximation}{{5.39}{110}{Finite Properties of the Cohen's d Estimator and a Variance-stabilizing Transformation}{equation.5.1.39}{}}
\newlabel{eq:cohen_d_expectation_approximation}{{5.40}{110}{Finite Properties of the Cohen's d Estimator and a Variance-stabilizing Transformation}{equation.5.1.40}{}}
\newlabel{eq:cohen_d_variance_approximation}{{5.41}{110}{Finite Properties of the Cohen's d Estimator and a Variance-stabilizing Transformation}{equation.5.1.41}{}}
\newlabel{eq:m1_squared}{{5.42}{110}{Finite Properties of the Cohen's d Estimator and a Variance-stabilizing Transformation}{equation.5.1.42}{}}
\newlabel{eq:m2_alternate_form}{{5.43}{110}{Finite Properties of the Cohen's d Estimator and a Variance-stabilizing Transformation}{equation.5.1.43}{}}
\newlabel{eq:variance_stabilizing_result}{{5.44}{110}{Finite Properties of the Cohen's d Estimator and a Variance-stabilizing Transformation}{equation.5.1.44}{}}
\newlabel{eq:quadratic taylor approximation}{{5.45}{110}{Finite Properties of the Cohen's d Estimator and a Variance-stabilizing Transformation}{equation.5.1.45}{}}
\citation{Laubscher1960-px}
\newlabel{eq:f_d_sqrt_n}{{5.46}{111}{Finite Properties of the Cohen's d Estimator and a Variance-stabilizing Transformation}{equation.5.1.46}{}}
\newlabel{eq:the_root_d_transformation}{{5.47}{111}{Finite Properties of the Cohen's d Estimator and a Variance-stabilizing Transformation}{equation.5.1.47}{}}
\newlabel{eq:the_final_transformation}{{5.48}{111}{Finite Properties of the Cohen's d Estimator and a Variance-stabilizing Transformation}{equation.5.1.48}{}}
\newlabel{eq:estimated_expection_of_d_hat}{{5.49}{111}{Finite Properties of the Cohen's d Estimator and a Variance-stabilizing Transformation}{equation.5.1.49}{}}
\newlabel{eq:bias_corrected_cohens_CSs}{{5.50}{112}{Finite Properties of the Cohen's d Estimator and a Variance-stabilizing Transformation}{equation.5.1.50}{}}
\newlabel{eq:bias_corrected_alternate_cohens_CSs}{{5.51}{112}{Finite Properties of the Cohen's d Estimator and a Variance-stabilizing Transformation}{equation.5.1.51}{}}
\newlabel{eq:plug_in_boundary}{{5.52}{112}{Finite Properties of the Cohen's d Estimator and a Variance-stabilizing Transformation}{equation.5.1.52}{}}
\newlabel{eq:transformed_cohens_CSs}{{5.53}{112}{Finite Properties of the Cohen's d Estimator and a Variance-stabilizing Transformation}{equation.5.1.53}{}}
\newlabel{eq:single_subject_transformed_residuals}{{5.54}{112}{Finite Properties of the Cohen's d Estimator and a Variance-stabilizing Transformation}{equation.5.1.54}{}}
\newlabel{eq:transformed_approximating_residuals}{{5.55}{113}{Finite Properties of the Cohen's d Estimator and a Variance-stabilizing Transformation}{equation.5.1.55}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.6}Three Algorithms for Computing Cohen's d CSs}{113}{subsection.5.1.6}}
\newlabel{sec:three_algorithms}{{5.1.6}{113}{Three Algorithms for Computing Cohen's d CSs}{subsection.5.1.6}{}}
\newlabel{alg:one}{{1}{113}{}{algorithm.1}{}}
\newlabel{alg:two}{{2}{114}{}{algorithm.2}{}}
\newlabel{alg:three}{{3}{115}{}{algorithm.3}{}}
\newlabel{eq:transformed_cohens_CSs_again}{{5.56}{116}{}{equation.5.1.56}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Methods}{116}{section.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Simulations}{116}{subsection.5.2.1}}
\newlabel{sec:cohen_simulations}{{5.2.1}{116}{Simulations}{subsection.5.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}2D Simulations}{116}{subsection.5.2.2}}
\newlabel{sec:cohen_2D_simulations}{{5.2.2}{116}{2D Simulations}{subsection.5.2.2}{}}
\citation{Cohen2013-it}
\newlabel{fig:linear_ramp_homo}{{5.2(a)}{117}{Subfigure 5 5.2(a)}{subfigure.5.2.1}{}}
\newlabel{sub@fig:linear_ramp_homo}{{(a)}{117}{Subfigure 5 5.2(a)\relax }{subfigure.5.2.1}{}}
\newlabel{fig:linear_ramp_hetero}{{5.2(b)}{117}{Subfigure 5 5.2(b)}{subfigure.5.2.2}{}}
\newlabel{sub@fig:linear_ramp_hetero}{{(b)}{117}{Subfigure 5 5.2(b)\relax }{subfigure.5.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces The two Cohen's $d$ effects corresponding to the linear ramp signal $\mu (\bm  {s})$. On the left, the subject-specific Gaussian noise field $\epsilon _{i}(\bm  {s})$ has a spatially constant standard deviation of 1, and therefore $d(\bm  {s}) = \mu (\bm  {s})$. On the right, $\epsilon _{i}(\bm  {s})$ had a spatially increasing standard deviation structure in the y-direction (from top-to-bottom), while remaining constant in the x-direction.\relax }}{117}{figure.caption.42}}
\newlabel{fig:linear_ramp_figures}{{5.2}{117}{The two Cohen's $d$ effects corresponding to the linear ramp signal $\mu (\bm {s})$. On the left, the subject-specific Gaussian noise field $\epsilon _{i}(\bm {s})$ has a spatially constant standard deviation of 1, and therefore $d(\bm {s}) = \mu (\bm {s})$. On the right, $\epsilon _{i}(\bm {s})$ had a spatially increasing standard deviation structure in the y-direction (from top-to-bottom), while remaining constant in the x-direction.\relax }{figure.caption.42}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {The Cohen's $d$ field $d(\bm {s})$ for the linear ramp signal $\mu (\bm {s})$ and homogeneous noise structure.}}}{117}{figure.caption.42}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {The Cohen's $d$ field $d(\bm {s})$ for the linear ramp signal $\mu (\bm {s})$ and heterogeneous noise structure.}}}{117}{figure.caption.42}}
\newlabel{fig:circle_homo}{{5.3(a)}{118}{Subfigure 5 5.3(a)}{subfigure.5.3.1}{}}
\newlabel{sub@fig:circle_homo}{{(a)}{118}{Subfigure 5 5.3(a)\relax }{subfigure.5.3.1}{}}
\newlabel{fig:circle_hetero}{{5.3(b)}{118}{Subfigure 5 5.3(b)}{subfigure.5.3.2}{}}
\newlabel{sub@fig:circle_hetero}{{(b)}{118}{Subfigure 5 5.3(b)\relax }{subfigure.5.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces The two Cohen's $d$ effects corresponding to the circular signal $\mu (\bm  {s})$. On the left, the subject-specific Gaussian noise field $\epsilon _{i}(\bm  {s})$ has a spatially constant standard deviation of 1, and therefore $d(\bm  {s}) = \mu (\bm  {s})$. On the right, $\epsilon _{i}(\bm  {s})$ had a spatially increasing standard deviation structure in the y-direction (from top-to-bottom), while remaining constant in the x-direction.\relax }}{118}{figure.caption.43}}
\newlabel{fig:circle_figures}{{5.3}{118}{The two Cohen's $d$ effects corresponding to the circular signal $\mu (\bm {s})$. On the left, the subject-specific Gaussian noise field $\epsilon _{i}(\bm {s})$ has a spatially constant standard deviation of 1, and therefore $d(\bm {s}) = \mu (\bm {s})$. On the right, $\epsilon _{i}(\bm {s})$ had a spatially increasing standard deviation structure in the y-direction (from top-to-bottom), while remaining constant in the x-direction.\relax }{figure.caption.43}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {The Cohen's $d$ field $d(\bm {s})$ for the circular signal $\mu (\bm {s})$ and homogeneous noise structure.}}}{118}{figure.caption.43}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {The Cohen's $d$ field $d(\bm {s})$ for the circular signal $\mu (\bm {s})$ and heterogeneous noise structure.}}}{118}{figure.caption.43}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}3D Simulations}{118}{subsection.5.2.3}}
\citation{Miller2016-hd}
\citation{Alfaro-Almagro2018-ip}
\citation{Hariri2002-ns}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Four of the Cohen's $d$ fields $d(\bm  {s})$ used for the 3D simulations. In (a) to (c), we show the Cohen's $d$ field for the three different spherical effects $\mu (\bm  {s})$ when Gaussian noise with spatially homogeneous standard deviation was added to the signal. In (d) we show the Cohen's $d$ field corresponding to the UK Biobank full mean and standard deviation images. Note that the colormap limits for the first three Cohen's $d$ effect-size images are from 0 to 1, while the colormap limits for the UK Biobank image is from -0.8 to 0.8.\relax }}{120}{table.caption.44}}
\newlabel{tbl:Cohens_d_3D_figures}{{5.4}{120}{Four of the Cohen's $d$ fields $d(\bm {s})$ used for the 3D simulations. In (a) to (c), we show the Cohen's $d$ field for the three different spherical effects $\mu (\bm {s})$ when Gaussian noise with spatially homogeneous standard deviation was added to the signal. In (d) we show the Cohen's $d$ field corresponding to the UK Biobank full mean and standard deviation images. Note that the colormap limits for the first three Cohen's $d$ effect-size images are from 0 to 1, while the colormap limits for the UK Biobank image is from -0.8 to 0.8.\relax }{table.caption.44}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.4}Application to Human Connectome Project Data}{121}{subsection.5.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Results}{121}{section.5.3}}
\newlabel{sec:simulation_results}{{5.3}{121}{Results}{section.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}2D Simulations}{121}{subsection.5.3.1}}
\newlabel{sec:2D_sim_results}{{5.3.1}{121}{2D Simulations}{subsection.5.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Coverage results for the linear ramp signal, with homogeneous (top row) and heterogeneous (bottom row) Gaussian noise structures. For large sample sizes the empirical coverage performance of all three algorithms was similar, hovering slightly above the nominal level in all simulations. For $N = 60$ the degree of over-coverage became larger for Algorithm \ref  {alg:one}., while empirical coverage for Algorithm \ref  {alg:two}.\ fell below the nominal target. Algorithm \ref  {alg:three}.\ performed best, with all results remaining particularly close to the nominal target level for simulations using a 95\% confidence level (right plots).\relax }}{122}{figure.caption.45}}
\newlabel{fig:Cohen_2D_sig_1_results}{{5.5}{122}{Coverage results for the linear ramp signal, with homogeneous (top row) and heterogeneous (bottom row) Gaussian noise structures. For large sample sizes the empirical coverage performance of all three algorithms was similar, hovering slightly above the nominal level in all simulations. For $N = 60$ the degree of over-coverage became larger for Algorithm \ref {alg:one}., while empirical coverage for Algorithm \ref {alg:two}.\ fell below the nominal target. Algorithm \ref {alg:three}.\ performed best, with all results remaining particularly close to the nominal target level for simulations using a 95\% confidence level (right plots).\relax }{figure.caption.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Coverage results for the circular signal, with homogeneous (top row) and heterogeneous (bottom row) Gaussian noise structures. All algorithms performed very well, and unlike the linear ramp, empirical coverage for all three methods converged towards the nominal level. For smaller sample sizes there was a slight degree of over-coverage, most noticeably for simulations using the 80\% nominal target. Overall, Algorithm \ref  {alg:two}.\ performed marginally better than the other two methods, and Algorithm \ref  {alg:one}.\ performed the worst.\relax }}{123}{figure.caption.46}}
\newlabel{fig:Cohen_2D_sig_2_results}{{5.6}{123}{Coverage results for the circular signal, with homogeneous (top row) and heterogeneous (bottom row) Gaussian noise structures. All algorithms performed very well, and unlike the linear ramp, empirical coverage for all three methods converged towards the nominal level. For smaller sample sizes there was a slight degree of over-coverage, most noticeably for simulations using the 80\% nominal target. Overall, Algorithm \ref {alg:two}.\ performed marginally better than the other two methods, and Algorithm \ref {alg:one}.\ performed the worst.\relax }{figure.caption.46}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}3D Simulations}{124}{subsection.5.3.2}}
\newlabel{sec:3d_sim_results}{{5.3.2}{124}{3D Simulations}{subsection.5.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Coverage results for the small sphere signal type, with homogeneous (top row) and heterogeneous (bottom row) Gaussian noise structures. In general, empirical coverage remained above the nominal level across all simulations, and for the 95\% confidence level (right plots), the results of all three methods fell close to the nominal target. All methods were robust as to whether the subject-level noise had homogeneous or heterogeneous variance structure. Because of this, there are minimal differences comparing the plots between both rows.\relax }}{126}{figure.caption.47}}
\newlabel{fig:Cohen_3D_sig_1_results}{{5.7}{126}{Coverage results for the small sphere signal type, with homogeneous (top row) and heterogeneous (bottom row) Gaussian noise structures. In general, empirical coverage remained above the nominal level across all simulations, and for the 95\% confidence level (right plots), the results of all three methods fell close to the nominal target. All methods were robust as to whether the subject-level noise had homogeneous or heterogeneous variance structure. Because of this, there are minimal differences comparing the plots between both rows.\relax }{figure.caption.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Coverage results for the large sphere signal type, with homogeneous (top row) and heterogeneous (bottom row) Gaussian noise structures. Compared with the small sphere results displayed in Fig.\ \ref  {fig:Cohen_3D_sig_2_results}, empirical coverage results were higher for all three methods here. Algorithm \ref  {alg:one}.\ suffered from a particularly large degree of over-coverage for simulations with a small sample size. Coverage performance for Algorithm \ref  {alg:two}.\ and Algorithm \ref  {alg:three}.\ was closer in resemblance to the corresponding small sphere results, with Algorithm \ref  {alg:two}.\ performing slightly better. This suggests that both of these methods are fairly robust to changes in the boundary length.\relax }}{127}{figure.caption.48}}
\newlabel{fig:Cohen_3D_sig_2_results}{{5.8}{127}{Coverage results for the large sphere signal type, with homogeneous (top row) and heterogeneous (bottom row) Gaussian noise structures. Compared with the small sphere results displayed in Fig.\ \ref {fig:Cohen_3D_sig_2_results}, empirical coverage results were higher for all three methods here. Algorithm \ref {alg:one}.\ suffered from a particularly large degree of over-coverage for simulations with a small sample size. Coverage performance for Algorithm \ref {alg:two}.\ and Algorithm \ref {alg:three}.\ was closer in resemblance to the corresponding small sphere results, with Algorithm \ref {alg:two}.\ performing slightly better. This suggests that both of these methods are fairly robust to changes in the boundary length.\relax }{figure.caption.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Coverage results for the multiple spheres signal type, with homogeneous (top row) and heterogeneous (bottom row) Gaussian noise structures. Algorithm \ref  {alg:two}.\ and Algorithm \ref  {alg:three}.\ both performed well, particularly for the 95\% confidence level, where coverage levels remained in the vicinity of the 95\% confidence interval of the nominal target. While Algorithm \ref  {alg:two}.\ was closer to the nominal target for $N = 60$, in some cases empirical coverage went slightly below the nominal level. In comparison to the other two methods, Algorithm \ref  {alg:one}.\ suffered from a large degree of over-coverage, which slightly improved as the sample size increased.\relax }}{128}{figure.caption.49}}
\newlabel{fig:Cohen_3D_sig_3_results}{{5.9}{128}{Coverage results for the multiple spheres signal type, with homogeneous (top row) and heterogeneous (bottom row) Gaussian noise structures. Algorithm \ref {alg:two}.\ and Algorithm \ref {alg:three}.\ both performed well, particularly for the 95\% confidence level, where coverage levels remained in the vicinity of the 95\% confidence interval of the nominal target. While Algorithm \ref {alg:two}.\ was closer to the nominal target for $N = 60$, in some cases empirical coverage went slightly below the nominal level. In comparison to the other two methods, Algorithm \ref {alg:one}.\ suffered from a large degree of over-coverage, which slightly improved as the sample size increased.\relax }{figure.caption.49}{}}
\citation{Cohen2013-it}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Coverage results for the UK Biobank signal type, where the full standard deviation image was used as the standard deviation of the subject-level noise fields. Covereage results here were similar to the results for the multiple spheres signal type shown in Fig.\ \ref  {fig:Cohen_3D_sig_3_results}. Once again, both Algorithm \ref  {alg:two}.\ and Algorithm \ref  {alg:three}.\ performed well, with empirical coverage rates hovering above the nominal target for large sample sizes, while results for Algorithm \ref  {alg:one}.\ came further above the nominal level.\relax }}{129}{figure.caption.50}}
\newlabel{fig:Cohen_3D_sig_4_results}{{5.10}{129}{Coverage results for the UK Biobank signal type, where the full standard deviation image was used as the standard deviation of the subject-level noise fields. Covereage results here were similar to the results for the multiple spheres signal type shown in Fig.\ \ref {fig:Cohen_3D_sig_3_results}. Once again, both Algorithm \ref {alg:two}.\ and Algorithm \ref {alg:three}.\ performed well, with empirical coverage rates hovering above the nominal target for large sample sizes, while results for Algorithm \ref {alg:one}.\ came further above the nominal level.\relax }{figure.caption.50}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Human Connectome Project}{130}{subsection.5.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces Slices views of the Cohen's $d$ Confidence Sets obtained from applying Algorithm \ref  {alg:three}.\ to the HCP working memory task data, using three Cohen's $d$ effect size thresholds, $c = 0.5, 0.8$ and $1.2$. The upper CS $\Ahatcp  $ is displayed in red, and the lower CS $\Ahatcm  $ in blue. Yellow voxels represent the point estimate set $\Ahatc  $, the best guess from the data of voxels that have surpassed the Cohen's $d$ threshold. The red upper CS has localized regions in the frontal gyrus, paracingulate gyrus, angular gyrus, cerebellum and precuneus which we can assert with 95\% confidence have attained (at least) a 0.5 Cohen's $d$ effect size.\relax }}{131}{table.caption.51}}
\newlabel{fig:HCP_Algorithm_3}{{5.11}{131}{Slices views of the Cohen's $d$ Confidence Sets obtained from applying Algorithm \ref {alg:three}.\ to the HCP working memory task data, using three Cohen's $d$ effect size thresholds, $c = 0.5, 0.8$ and $1.2$. The upper CS $\Ahatcp $ is displayed in red, and the lower CS $\Ahatcm $ in blue. Yellow voxels represent the point estimate set $\Ahatc $, the best guess from the data of voxels that have surpassed the Cohen's $d$ threshold. The red upper CS has localized regions in the frontal gyrus, paracingulate gyrus, angular gyrus, cerebellum and precuneus which we can assert with 95\% confidence have attained (at least) a 0.5 Cohen's $d$ effect size.\relax }{table.caption.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces Comparing the upper Confidence Sets computed with Algorithm 3.\ on the HCP working memory task data (same slice views as Fig.\ \ref  {fig:HCP_Algorithm_3}) with the thresholded $t$-statistic results obtained by applying a traditional group-level one-sample $t$-test, voxelwise $p < 0.05$ FWE correction (green-yellow voxels). While the thresholded statistic map contains a single cluster covering a sizable portion of the parietal lobe across both hemispheres (axial slices), the upper CSs have localized the precise areas of the precuneus and anglur gyrus where we can confidently declare a Cohen's $d$ effect size of at least 0.5. This demonstrates how the CSs can provide improved spatial specificity in determining regions where practically significant activation has occured.\relax }}{132}{table.caption.52}}
\newlabel{fig:HCP_Algorithm_3_vs_GLM}{{5.12}{132}{Comparing the upper Confidence Sets computed with Algorithm 3.\ on the HCP working memory task data (same slice views as Fig.\ \ref {fig:HCP_Algorithm_3}) with the thresholded $t$-statistic results obtained by applying a traditional group-level one-sample $t$-test, voxelwise $p < 0.05$ FWE correction (green-yellow voxels). While the thresholded statistic map contains a single cluster covering a sizable portion of the parietal lobe across both hemispheres (axial slices), the upper CSs have localized the precise areas of the precuneus and anglur gyrus where we can confidently declare a Cohen's $d$ effect size of at least 0.5. This demonstrates how the CSs can provide improved spatial specificity in determining regions where practically significant activation has occured.\relax }{table.caption.52}{}}
\citation{Poldrack2017-rr,Cremers2017-qk}
\citation{Reddan2017-rk,Button2013-ni}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Discussion}{133}{section.5.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Spatial Inference on Cohen's d Effect Size}{133}{subsection.5.4.1}}
\citation{Cohen2013-it}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Three Algorithms for Cohen's d Confidence Sets}{135}{subsection.5.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.3}Limitations}{136}{subsection.5.4.3}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Conclusion}{136}{section.5.5}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion and Future Work}{137}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:conclusion}{{6}{137}{Conclusion and Future Work}{chapter.6}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Software Comparison Supplementary Material}{138}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Percentage BOLD change Maps}{138}{section.A.1}}
\newlabel{App:SC_supplementary_BOLD}{{A.1}{138}{Percentage BOLD change Maps}{section.A.1}{}}
\newlabel{eq:SC_sup_data_scale}{{A.1}{138}{Percentage BOLD change Maps}{equation.A.1.1}{}}
\newlabel{eq:SC_sup_design_scale}{{A.2}{138}{Percentage BOLD change Maps}{equation.A.1.2}{}}
\citation{Chen2017-sb}
\citation{Nichols2012-rx}
\newlabel{eq:SC_sup_contrast_scale}{{A.3}{139}{Percentage BOLD change Maps}{equation.A.1.3}{}}
\newlabel{eq:SC_sup_ideal_scale}{{A.4}{139}{Percentage BOLD change Maps}{equation.A.1.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Partial $R^{2}$ Maps}{140}{section.A.2}}
\newlabel{App:SC_supplementary_R2}{{A.2}{140}{Partial $R^{2}$ Maps}{section.A.2}{}}
\newlabel{eq:SC_sup_F_to_R}{{A.5}{140}{Partial $R^{2}$ Maps}{equation.A.2.5}{}}
\newlabel{eq:SC_sup_R_to_F}{{A.6}{140}{Partial $R^{2}$ Maps}{equation.A.2.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.3}Supplementary Figures}{141}{section.A.3}}
\newlabel{App:SC_supplementary_figures}{{A.3}{141}{Supplementary Figures}{section.A.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces Registration Quality Control: Mean and standard deviation of anatomical and mean functional images\relax }}{142}{figure.caption.53}}
\newlabel{fig:SC_supp_Registration}{{A.1}{142}{Registration Quality Control: Mean and standard deviation of anatomical and mean functional images\relax }{figure.caption.53}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces ds000001 inter-software comparisons, 5\% FWE clusterwise inference\relax }}{143}{figure.caption.54}}
\newlabel{fig:SC_supp_thresh_ds000001}{{A.2}{143}{ds000001 inter-software comparisons, 5\% FWE clusterwise inference\relax }{figure.caption.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.3}{\ignorespaces ds000001 inter-software comparisons, 5\% FWE clusterwise permutation inference\relax }}{144}{figure.caption.55}}
\newlabel{fig:SC_supp_perm_thresh_ds000001}{{A.3}{144}{ds000001 inter-software comparisons, 5\% FWE clusterwise permutation inference\relax }{figure.caption.55}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.4}{\ignorespaces ds000109 inter-software comparisons, 5\% FWE clusterwise inference\relax }}{145}{figure.caption.56}}
\newlabel{fig:SC_supp_thresh_ds000109}{{A.4}{145}{ds000109 inter-software comparisons, 5\% FWE clusterwise inference\relax }{figure.caption.56}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.5}{\ignorespaces ds000109 inter-software comparisons, 5\% FWE clusterwise permutation inference\relax }}{146}{figure.caption.57}}
\newlabel{fig:SC_supp_perm_thresh_ds000109}{{A.5}{146}{ds000109 inter-software comparisons, 5\% FWE clusterwise permutation inference\relax }{figure.caption.57}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.6}{\ignorespaces ds000120 inter-software comparisons, 5\% FWE clusterwise inference\relax }}{147}{figure.caption.58}}
\newlabel{fig:SC_supp_thresh_ds000120}{{A.6}{147}{ds000120 inter-software comparisons, 5\% FWE clusterwise inference\relax }{figure.caption.58}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.7}{\ignorespaces ds000001 inter-software comparisons, $t$-statistic maps\relax }}{148}{figure.caption.59}}
\newlabel{fig:SC_supp_unthresh_ds000001}{{A.7}{148}{ds000001 inter-software comparisons, $t$-statistic maps\relax }{figure.caption.59}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.8}{\ignorespaces ds000001 inter-software comparisons, $t$-statistic maps from permutation\relax }}{149}{figure.caption.60}}
\newlabel{fig:SC_supp_perm_unthresh_ds000001}{{A.8}{149}{ds000001 inter-software comparisons, $t$-statistic maps from permutation\relax }{figure.caption.60}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.9}{\ignorespaces ds000109 inter-software comparisons, $t$-statistic maps\relax }}{150}{figure.caption.61}}
\newlabel{fig:SC_supp_unthresh_ds000109}{{A.9}{150}{ds000109 inter-software comparisons, $t$-statistic maps\relax }{figure.caption.61}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.10}{\ignorespaces ds000109 inter-software comparisons, $t$-statistic maps from permutation\relax }}{151}{figure.caption.62}}
\newlabel{fig:SC_supp_perm_unthresh_ds000109}{{A.10}{151}{ds000109 inter-software comparisons, $t$-statistic maps from permutation\relax }{figure.caption.62}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.11}{\ignorespaces ds0001020 inter-software comparisons, $F$-statistic maps\relax }}{152}{figure.caption.63}}
\newlabel{fig:SC_supp_unthresh_ds000120}{{A.11}{152}{ds0001020 inter-software comparisons, $F$-statistic maps\relax }{figure.caption.63}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.12}{\ignorespaces ds000120 inter-software comparisons, Euler characeristic and cluster count curves for $F$-statistic maps\relax }}{153}{figure.caption.64}}
\newlabel{fig:SC_supp_ECs}{{A.12}{153}{ds000120 inter-software comparisons, Euler characeristic and cluster count curves for $F$-statistic maps\relax }{figure.caption.64}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.13}{\ignorespaces Bland-Altman percent BOLD comparisons\relax }}{154}{figure.caption.65}}
\newlabel{fig:SC_supp_BOLD_BAs}{{A.13}{154}{Bland-Altman percent BOLD comparisons\relax }{figure.caption.65}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.14}{\ignorespaces ds000120 $R^{2}$ comparisons\relax }}{155}{figure.caption.66}}
\newlabel{fig:SC_supp_BOLD_BAs_ds000120}{{A.14}{155}{ds000120 $R^{2}$ comparisons\relax }{figure.caption.66}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}\%BOLD Confidence Sets Supplementary Material}{156}{appendix.B}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Supplementary Human Connectome Project Results}{156}{section.B.1}}
\newlabel{sec:supp_CI_figs}{{B.1}{156}{Supplementary Human Connectome Project Results}{section.B.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Supplementary Tables}{156}{section.B.2}}
\newlabel{sec:supp_CI_tabs}{{B.2}{156}{Supplementary Tables}{section.B.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.1}{\ignorespaces Comparing the upper Confidences Sets for the HCP working memory task data (same slice views as Fig. \ref  {tbl:HCP_results_one}) with the thresholded $t$-statistic results obtained by applying a traditional group-level one-sample $t$-test, voxelwise $p < 0.05$ FWE correction (green-yellow voxels). While over 25,000 voxels were determined as statistically significant with the standard inference method, less than 5,000 voxels were asserted to have at least a 1.0\% BOLD change by the CSs. In particular, the two statistically significant clusters spanning the left and right side of the frontal lobe contained almost no voxels with a practical effect size of over 1.5\% BOLD change.\relax }}{157}{table.caption.67}}
\newlabel{tbl:Supp_HCP_results_one}{{B.1}{157}{Comparing the upper Confidences Sets for the HCP working memory task data (same slice views as Fig. \ref {tbl:HCP_results_one}) with the thresholded $t$-statistic results obtained by applying a traditional group-level one-sample $t$-test, voxelwise $p < 0.05$ FWE correction (green-yellow voxels). While over 25,000 voxels were determined as statistically significant with the standard inference method, less than 5,000 voxels were asserted to have at least a 1.0\% BOLD change by the CSs. In particular, the two statistically significant clusters spanning the left and right side of the frontal lobe contained almost no voxels with a practical effect size of over 1.5\% BOLD change.\relax }{table.caption.67}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.2}{\ignorespaces Comparing the upper Confidences Sets for the HCP working memory task data (same slice views as Fig. \ref  {tbl:HCP_results_two}) with the thresholded $t$-statistic results obtained by applying a traditional group-level one-sample $t$-test, voxelwise $p < 0.05$ FWE correction (green-yellow voxels). While one large statistically significant cluster covers the supramarginal gyrus, angular gyrus and precuneous, the CSs localize the precise areas with practically significant effect sizes within each of these regions.\relax }}{158}{table.caption.68}}
\newlabel{tbl:Supp_HCP_results_two}{{B.2}{158}{Comparing the upper Confidences Sets for the HCP working memory task data (same slice views as Fig. \ref {tbl:HCP_results_two}) with the thresholded $t$-statistic results obtained by applying a traditional group-level one-sample $t$-test, voxelwise $p < 0.05$ FWE correction (green-yellow voxels). While one large statistically significant cluster covers the supramarginal gyrus, angular gyrus and precuneous, the CSs localize the precise areas with practically significant effect sizes within each of these regions.\relax }{table.caption.68}{}}
\@writefile{lot}{\contentsline {table}{\numberline {B.1}{\ignorespaces Empirical coverage results for the 2D simulations using nominal (nom.) coverage levels $1-\alpha = 80\%, 90\%$ and $95\%$. Results are shown for applying the Wild $t$-Bootstrap method to the residual field along the estimated boundary $\dAhatc  $ (top) and the true boundary $\dAc  $ (bottom).\relax }}{159}{table.caption.69}}
\newlabel{tbl:supp_CI_2D_results}{{B.1}{159}{Empirical coverage results for the 2D simulations using nominal (nom.) coverage levels $1-\alpha = 80\%, 90\%$ and $95\%$. Results are shown for applying the Wild $t$-Bootstrap method to the residual field along the estimated boundary $\dAhatc $ (top) and the true boundary $\dAc $ (bottom).\relax }{table.caption.69}{}}
\@writefile{lot}{\contentsline {table}{\numberline {B.2}{\ignorespaces Empirical coverage results for the 3D simulations using nominal (nom.) coverage levels $1-\alpha = 80\%, 90\%$ and $95\%$. Results are shown for applying the Wild $t$-Bootstrap method to the residual field along the estimated boundary $\dAhatc  $ (top) and the true boundary $\dAc  $ (bottom).\relax }}{160}{table.caption.70}}
\newlabel{tbl:supp_CI_3D_results}{{B.2}{160}{Empirical coverage results for the 3D simulations using nominal (nom.) coverage levels $1-\alpha = 80\%, 90\%$ and $95\%$. Results are shown for applying the Wild $t$-Bootstrap method to the residual field along the estimated boundary $\dAhatc $ (top) and the true boundary $\dAc $ (bottom).\relax }{table.caption.70}{}}
\bibstyle{plainnat}
\bibdata{perm}
\@writefile{toc}{\contentsline {chapter}{\numberline {C}Cohen's d Confidence Sets Supplementary Material}{162}{appendix.C}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {C.1}Supplementary Human Connectome Project Results}{162}{section.C.1}}
\newlabel{sec:Cohen_supp_CI_figs}{{C.1}{162}{Supplementary Human Connectome Project Results}{section.C.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {C.1}{\ignorespaces Slices views of the Cohen's $d$ Confidence Sets obtained from applying Algorithm \ref  {alg:one}.\ to the HCP working memory task data, using three Cohen's $d$ effect size thresholds, $c = 0.5, 0.8$ and $1.2$. Comparing with Fig.\ \ref  {fig:HCP_Algorithm_3} and Fig.\ \ref  {fig:HCP_Algorithm_2}, the CSs presented here are slightly more conservative than the corresponding CSs obtained with Algorithm \ref  {alg:two}. and Algorithm \ref  {alg:three}.\ (in the sense that the red upper CSs here are smaller, and blue lower CSs are larger). This is consistent with the simulation results obtained in Section \ref  {sec:2D_sim_results} and \ref  {sec:3d_sim_results}, where the empirical coverage for Algorithm \ref  {alg:one}.\ was consistently larger than the other two methods.\relax }}{163}{table.caption.72}}
\newlabel{fig:HCP_Algorithm_1}{{C.1}{163}{Slices views of the Cohen's $d$ Confidence Sets obtained from applying Algorithm \ref {alg:one}.\ to the HCP working memory task data, using three Cohen's $d$ effect size thresholds, $c = 0.5, 0.8$ and $1.2$. Comparing with Fig.\ \ref {fig:HCP_Algorithm_3} and Fig.\ \ref {fig:HCP_Algorithm_2}, the CSs presented here are slightly more conservative than the corresponding CSs obtained with Algorithm \ref {alg:two}. and Algorithm \ref {alg:three}.\ (in the sense that the red upper CSs here are smaller, and blue lower CSs are larger). This is consistent with the simulation results obtained in Section \ref {sec:2D_sim_results} and \ref {sec:3d_sim_results}, where the empirical coverage for Algorithm \ref {alg:one}.\ was consistently larger than the other two methods.\relax }{table.caption.72}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {C.2}{\ignorespaces Slices views of the Cohen's $d$ Confidence Sets obtained from applying Algorithm \ref  {alg:two}.\ to the HCP working memory task data, using three Cohen's $d$ effect size thresholds, $c = 0.5, 0.8$ and $1.2$. Comparing with Fig.\ \ref  {fig:HCP_Algorithm_3}, the upper and lower CSs presented here are almost identical to the corresponding CSs obtained with Algorithm \ref  {alg:three}.\relax }}{164}{table.caption.73}}
\newlabel{fig:HCP_Algorithm_2}{{C.2}{164}{Slices views of the Cohen's $d$ Confidence Sets obtained from applying Algorithm \ref {alg:two}.\ to the HCP working memory task data, using three Cohen's $d$ effect size thresholds, $c = 0.5, 0.8$ and $1.2$. Comparing with Fig.\ \ref {fig:HCP_Algorithm_3}, the upper and lower CSs presented here are almost identical to the corresponding CSs obtained with Algorithm \ref {alg:three}.\relax }{table.caption.73}{}}
\bibcite{Abraham2014-ap}{{1}{2014}{{Abraham et~al.}}{{Abraham, Pedregosa, Eickenberg, Gervais, Mueller, Kossaifi, Gramfort, Thirion, and Varoquaux}}}
\bibcite{Adelman1987-hs}{{2}{1987}{{Adelman and {Others}}}{{}}}
\bibcite{Alexander2007-ut}{{3}{2007}{{Alexander et~al.}}{{Alexander, Lee, Lazar, and Field}}}
\bibcite{Alfaro-Almagro2018-ip}{{4}{2018}{{Alfaro-Almagro et~al.}}{{Alfaro-Almagro, Jenkinson, Bangerter, Andersson, Griffanti, Douaud, Sotiropoulos, Jbabdi, Hernandez-Fernandez, Vallee, Vidaurre, Webster, McCarthy, Rorden, Daducci, Alexander, Zhang, Dragonu, Matthews, Miller, and Smith}}}
\bibcite{Anatomy1918-do}{{5}{1918}{{Anatomy}}{{}}}
\bibcite{Andersson2007-lc}{{6}{2007}{{Andersson et~al.}}{{Andersson, Jenkinson, Smith, and {Others}}}}
\bibcite{Azevedo2009-qj}{{7}{2009}{{Azevedo et~al.}}{{Azevedo, Carvalho, Grinberg, Farfel, Ferretti, Leite, Filho, Lent, and Herculano-Houzel}}}
\bibcite{Bandettini1992-jt}{{8}{1992}{{Bandettini et~al.}}{{Bandettini, Wong, Hinks, Tikofsky, and Hyde}}}
\bibcite{Barch2013-kk}{{9}{2013}{{Barch et~al.}}{{Barch, Burgess, Harms, Petersen, Schlaggar, Corbetta, Glasser, Curtiss, Dixit, Feldt, Nolan, Bryant, Hartley, Footer, Bjork, Poldrack, Smith, Johansen-Berg, Snyder, Van~Essen, and {WU-Minn HCP Consortium}}}}
\bibcite{Benjamini1995-yy}{{10}{1995}{{Benjamini and Hochberg}}{{}}}
\bibcite{Bennett2009-fh}{{11}{2009}{{Bennett et~al.}}{{Bennett, Miller, and Wolford}}}
\bibcite{Bennett1964-vx}{{12}{1964}{{Bennett et~al.}}{{Bennett, Diamond, Krech, and Rosenzweig}}}
\bibcite{Bowring2018-wp}{{13}{2018{a}}{{Bowring et~al.}}{{Bowring, Maumet, and Nichols}}}
\bibcite{Bowring2018-jp}{{14}{2018{b}}{{Bowring et~al.}}{{Bowring, Maumet, and Nichols}}}
\bibcite{Bowring2019-fc}{{15}{2019{a}}{{Bowring et~al.}}{{Bowring, Maumet, and Nichols}}}
\bibcite{BOWRING2019116187}{{16}{2019{b}}{{Bowring et~al.}}{{Bowring, Telschow, Schwartzman, and Nichols}}}
\bibcite{Brett2017-zb}{{17}{2017}{{Brett et~al.}}{{Brett, Hanke, C{\^o}t{\'e}, Markiewicz, Ghosh, Wassermann, Gerhard, Larson, Lee, Halchenko, Kastman, M, Morency, {moloney}, Rokem, Cottaar, Millman, {jaeilepp}, Gramfort, Vincent, McCarthy, van~den Bosch, Subramaniam, Nichols, {embaker}, {markhymers}, {chaselgrove}, {Basile}, Oosterhof, and Nimmo-Smith}}}
\bibcite{Brookes2011-cj}{{18}{2011}{{Brookes et~al.}}{{Brookes, Woolrich, Luckhoo, Price, Hale, Stephenson, Barnes, Smith, and Morris}}}
\bibcite{Button2013-ni}{{19}{2013}{{Button et~al.}}{{Button, Ioannidis, Mokrysz, Nosek, Flint, Robinson, and Munaf{\`o}}}}
\bibcite{Buxton2012-ly}{{20}{2012}{{Buxton}}{{}}}
\bibcite{Carp2012-ph}{{21}{2012}{{Carp}}{{}}}
\bibcite{Carp2013-cm}{{22}{2013}{{Carp}}{{}}}
\bibcite{Chen2017-sb}{{23}{2017}{{Chen et~al.}}{{Chen, Taylor, and Cox}}}
\bibcite{Chen2018-am}{{24}{2018}{{Chen et~al.}}{{Chen, Cox, Glen, Rajendra, Reynolds, and Taylor}}}
\bibcite{Chernozhukov2013-wz}{{25}{2013}{{Chernozhukov et~al.}}{{Chernozhukov, Chetverikov, and Kato}}}
\bibcite{Chumbley2009-ce}{{26}{2009}{{Chumbley and Friston}}{{}}}
\bibcite{Cohen2013-it}{{27}{2013}{{Cohen}}{{}}}
\bibcite{Cox1996-nu}{{28}{1996}{{Cox}}{{}}}
\bibcite{Cox2017-wr}{{29}{2017{a}}{{Cox et~al.}}{{Cox, Chen, Glen, Reynolds, and Taylor}}}
\bibcite{Cox2017-ys}{{30}{2017{b}}{{Cox et~al.}}{{Cox, Chen, Glen, Reynolds, and Taylor}}}
\bibcite{Cremers2017-qk}{{31}{2017}{{Cremers et~al.}}{{Cremers, Wager, and Yarkoni}}}
\bibcite{David2013-iz}{{32}{2013}{{David et~al.}}{{David, Ware, Chu, Loftus, Fusar-Poli, Radua, Munaf{\`o}, and Ioannidis}}}
\bibcite{Davidson2008-qh}{{33}{2008}{{Davidson and Flachaire}}{{}}}
\bibcite{Diamond1964-cu}{{34}{1964}{{Diamond et~al.}}{{Diamond, Krech, and Rosenzweig}}}
\bibcite{Eklund2016-ak}{{35}{2016}{{Eklund et~al.}}{{Eklund, Nichols, and Knutsson}}}
\bibcite{Ellis2019-zi}{{36}{2019}{{Ellis et~al.}}{{Ellis, Baldassano, Schapiro, Cai, and Cohen}}}
\bibcite{Engel2013-nq}{{37}{2013}{{Engel and Burton}}{{}}}
\bibcite{Erin_D_Foster2017-au}{{38}{2017}{{Erin D.~Foster}}{{}}}
\bibcite{Flandin2019-qx}{{39}{2019}{{Flandin and Friston}}{{}}}
\bibcite{Fomina2015-ha}{{40}{2015}{{Fomina et~al.}}{{Fomina, Hohmann, Scholkopf, and Grosse-Wentrup}}}
\bibcite{Fox2015-ds}{{41}{2015}{{Fox and Alterman}}{{}}}
\bibcite{Friston1998-jl}{{42}{1998}{{Friston et~al.}}{{Friston, Fletcher, Josephs, Holmes, Rugg, and Turner}}}
\bibcite{Genovese2002-xm}{{43}{2002}{{Genovese et~al.}}{{Genovese, Lazar, and Nichols}}}
\bibcite{Glasser2013-qc}{{44}{2013}{{Glasser et~al.}}{{Glasser, Sotiropoulos, Wilson, Coalson, Fischl, Andersson, Xu, Jbabdi, Webster, Polimeni, Van~Essen, Jenkinson, and {WU-Minn HCP Consortium}}}}
\bibcite{Glatard2015-ml}{{45}{2015}{{Glatard et~al.}}{{Glatard, Lewis, da~Silva, Adalat, Beck, Lepage, and {Others}}}}
\bibcite{Glover2011-at}{{46}{2011}{{Glover}}{{}}}
\bibcite{Gonzalez-Castillo2012-do}{{47}{2012}{{Gonzalez-Castillo et~al.}}{{Gonzalez-Castillo, Saad, Handwerker, Inati, Brenowitz, and Bandettini}}}
\bibcite{Gorgolewski2016-fk}{{48}{2016}{{Gorgolewski and Poldrack}}{{}}}
\bibcite{Gorgolewski2015-vs}{{49}{2015}{{Gorgolewski et~al.}}{{Gorgolewski, Varoquaux, Rivera, Schwarz, Ghosh, Maumet, Sochat, Nichols, Poldrack, Poline, Yarkoni, and Margulies}}}
\bibcite{Gorgolewski2016-nf}{{50}{2016}{{Gorgolewski et~al.}}{{Gorgolewski, Auer, Calhoun, Craddock, Das, Duff, Flandin, Ghosh, Glatard, Halchenko, Handwerker, Hanke, Keator, Li, Michael, Maumet, Nichols, Nichols, Pellman, Poline, Rokem, Schaefer, Sochat, Triplett, Turner, Varoquaux, and Poldrack}}}
\bibcite{Goutte2000-vd}{{51}{2000}{{Goutte et~al.}}{{Goutte, Nielsen, and Hansen}}}
\bibcite{Gronenschild2012-gf}{{52}{2012}{{Gronenschild et~al.}}{{Gronenschild, Habets, Jacobs, Mengelers, Rozendaal, van Os, and Marcelis}}}
\bibcite{Hargreaves2012-dz}{{53}{2012}{{Hargreaves and Klimas}}{{}}}
\bibcite{Hariri2002-ns}{{54}{2002}{{Hariri et~al.}}{{Hariri, Tessitore, Mattay, Fera, and Weinberger}}}
\bibcite{Hong2019-qr}{{55}{2019}{{Hong et~al.}}{{Hong, Yoo, Han, Wager, and Woo}}}
\bibcite{Hunter2007-nu}{{56}{2007}{{Hunter}}{{}}}
\bibcite{Ioannidis2005-sy}{{57}{2005}{{Ioannidis}}{{}}}
\bibcite{Ioannidis2014-yn}{{58}{2014}{{Ioannidis et~al.}}{{Ioannidis, Munaf{\`o}, Fusar-Poli, Nosek, and David}}}
\bibcite{Jenkinson2002-qf}{{59}{2002}{{Jenkinson et~al.}}{{Jenkinson, Bannister, Brady, and Smith}}}
\bibcite{Jenkinson2012-wh}{{60}{2012}{{Jenkinson et~al.}}{{Jenkinson, Beckmann, Behrens, Woolrich, and Smith}}}
\bibcite{Kalia2013-bv}{{61}{2013}{{Kalia et~al.}}{{Kalia, Sankar, and Lozano}}}
\bibcite{Kluyver2016-yl}{{62}{2016}{{Kluyver et~al.}}{{Kluyver, Ragan-Kelley, P{\'e}rez, Granger, Bussonnier, Frederic, Kelley, Hamrick, Grout, Corlay, and {Others}}}}
\bibcite{Kwong1992-uq}{{63}{1992}{{Kwong et~al.}}{{Kwong, Belliveau, Chesler, Goldberg, Weisskoff, Poncelet, Kennedy, Hoppel, Cohen, and Turner}}}
\bibcite{Laubscher1960-px}{{64}{1960}{{Laubscher}}{{}}}
\bibcite{Lee2013-kn}{{65}{2013}{{Lee et~al.}}{{Lee, Smyser, and Shimony}}}
\bibcite{Lee2012-di}{{66}{2012}{{Lee et~al.}}{{Lee, Hacker, Snyder, Corbetta, Zhang, Leuthardt, and Shimony}}}
\bibcite{Lindquist2009-fs}{{67}{2009}{{Lindquist et~al.}}{{Lindquist, Meng~Loh, Atlas, and Wager}}}
\bibcite{Lopez-Munoz2006-zk}{{68}{2006}{{L{\'o}pez-Mu{\~n}oz et~al.}}{{L{\'o}pez-Mu{\~n}oz, Boya, and Alamo}}}
\bibcite{Lund2005-sf}{{69}{2005}{{Lund et~al.}}{{Lund, N{\o }rgaard, Rostrup, Rowe, and Paulson}}}
\bibcite{Matthews2006-sl}{{70}{2006}{{Matthews et~al.}}{{Matthews, Honey, and Bullmore}}}
\bibcite{Maumet2016-se}{{71}{2016}{{Maumet et~al.}}{{Maumet, Auer, Bowring, Chen, Das, Flandin, Ghosh, Glatard, Gorgolewski, Helmer, Jenkinson, Keator, Nichols, Poline, Reynolds, Sochat, Turner, and Nichols}}}
\bibcite{McEvoy2009-zx}{{72}{2009}{{McEvoy et~al.}}{{McEvoy, Fennema-Notestine, Cooper~Roddey, Hagler, Holland, Karow, Pung, Brewer, and Dale}}}
\bibcite{McKinney2010-dv}{{73}{2010}{{McKinney and {Others}}}{{}}}
\bibcite{Mechelli2005-dn}{{74}{2005}{{Mechelli et~al.}}{{Mechelli, Price, Friston, and Ashburner}}}
\bibcite{Meehl1967-ij}{{75}{1967}{{Meehl}}{{}}}
\bibcite{Miller2016-hd}{{76}{2016}{{Miller et~al.}}{{Miller, Alfaro-Almagro, Bangerter, Thomas, Yacoub, Xu, Bartsch, Jbabdi, Sotiropoulos, Andersson, Griffanti, Douaud, Okell, Weale, Dragonu, Garratt, Hudson, Collins, Jenkinson, Matthews, and Smith}}}
\bibcite{Mohamed2014-gl}{{77}{2014}{{Mohamed}}{{}}}
\bibcite{Moran2012-cw}{{78}{2012}{{Moran et~al.}}{{Moran, Jolly, and Mitchell}}}
\bibcite{Moussa2012-bl}{{79}{2012}{{Moussa et~al.}}{{Moussa, Steen, Laurienti, and Hayasaka}}}
\bibcite{Mueller2017-pn}{{80}{2017}{{Mueller et~al.}}{{Mueller, Lepsien, M{\"o}ller, and Lohmann}}}
\bibcite{Nichols2012-rx}{{81}{2012}{{Nichols}}{{}}}
\bibcite{Nichols2002-kf}{{82}{2002}{{Nichols and Holmes}}{{}}}
\bibcite{Nielsen2014-vl}{{83}{2014}{{Nielsen and Smith}}{{}}}
\bibcite{Ogawa1990-it}{{84}{1990}{{Ogawa et~al.}}{{Ogawa, Lee, Kay, and Tank}}}
\bibcite{Ogawa1992-af}{{85}{1992}{{Ogawa et~al.}}{{Ogawa, Tank, Menon, Ellermann, Kim, Merkle, and Ugurbil}}}
\bibcite{Olszowy2019-cz}{{86}{2019}{{Olszowy et~al.}}{{Olszowy, Aston, Rua, and Williams}}}
\bibcite{Open_Science_Collaboration2015-gr}{{87}{2015}{{Open Science Collaboration}}{{}}}
\bibcite{Padmanabhan2011-dc}{{88}{2011}{{Padmanabhan et~al.}}{{Padmanabhan, Geier, Ordaz, Teslovich, and Luna}}}
\bibcite{Parker_Jones2017-ld}{{89}{2017}{{Parker~Jones et~al.}}{{Parker~Jones, Voets, Adcock, Stacey, and Jbabdi}}}
\bibcite{Penny2011-uk}{{90}{2011}{{Penny et~al.}}{{Penny, Friston, Ashburner, Kiebel, and Nichols}}}
\bibcite{Poldrack2011-bw}{{91}{2011}{{Poldrack et~al.}}{{Poldrack, Mumford, and Nichols}}}
\bibcite{Poldrack2017-rr}{{92}{2017}{{Poldrack et~al.}}{{Poldrack, Baker, Durnez, Gorgolewski, Matthews, Munaf{\`o}, Nichols, Poline, Vul, and Yarkoni}}}
\bibcite{Reddan2017-rk}{{93}{2017}{{Reddan et~al.}}{{Reddan, Lindquist, and Wager}}}
\bibcite{Rozeboom1960-dp}{{94}{1960}{{Rozeboom}}{{}}}
\bibcite{Schonberg2012-oo}{{95}{2012}{{Schonberg et~al.}}{{Schonberg, Fox, Mumford, Congdon, Trepel, and Poldrack}}}
\bibcite{Searle2009-ku}{{96}{2009}{{Searle et~al.}}{{Searle, Casella, and McCulloch}}}
\bibcite{Skudlarski1999-ao}{{97}{1999}{{Skudlarski et~al.}}{{Skudlarski, Constable, and Gore}}}
\bibcite{Smith2002-vw}{{98}{2002}{{Smith}}{{}}}
\bibcite{Smith2009-dm}{{99}{2009}{{Smith et~al.}}{{Smith, Fox, Miller, Glahn, Fox, Mackay, Filippini, Watkins, Toro, Laird, and Beckmann}}}
\bibcite{Smith2013-ul}{{100}{2013}{{Smith et~al.}}{{Smith, Beckmann, Andersson, Auerbach, Bijsterbosch, Douaud, Duff, Feinberg, Griffanti, Harms, Kelly, Laumann, Miller, Moeller, Petersen, Power, Salimi-Khorshidi, Snyder, Vu, Woolrich, Xu, Yacoub, U{\u g}urbil, Van~Essen, Glasser, and {WU-Minn HCP Consortium}}}}
\bibcite{Soares2013-mh}{{101}{2013}{{Soares et~al.}}{{Soares, Marques, Alves, and Sousa}}}
\bibcite{Sommerfeld2018-zl}{{102}{2018}{{Sommerfeld et~al.}}{{Sommerfeld, Sain, and Schwartzman}}}
\bibcite{Sperling2014-sy}{{103}{2014}{{Sperling et~al.}}{{Sperling, Rentz, Johnson, Karlawish, Donohue, Salmon, and Aisen}}}
\bibcite{Stacey2008-qx}{{104}{2008}{{Stacey and Litt}}{{}}}
\bibcite{Strother2002-sz}{{105}{2002}{{Strother et~al.}}{{Strother, Anderson, Hansen, Kjems, Kustra, Sidtis, Frutiger, Muley, LaConte, and Rottenberg}}}
\bibcite{Tavor2016-pd}{{106}{2016}{{Tavor et~al.}}{{Tavor, Parker~Jones, Mars, Smith, Behrens, and Jbabdi}}}
\bibcite{Telschow2019-lg}{{107}{2019}{{Telschow and Schwartzman}}{{}}}
\bibcite{UIudag2009-nm}{{108}{2009}{{UIudag et~al.}}{{UIudag, M{\"u}ller-Bierl, and Ugurbil}}}
\bibcite{Wager2009-gm}{{109}{2009}{{Wager et~al.}}{{Wager, Lindquist, Nichols, Kober, and Van~Snellenberg}}}
\bibcite{Walt2011-db}{{110}{2011}{{Walt et~al.}}{{Walt, Colbert, and Varoquaux}}}
\bibcite{Winkler2016-mw}{{111}{2016}{{Winkler et~al.}}{{Winkler, Ridgway, Douaud, Nichols, and Smith}}}
\bibcite{Woo2014-ji}{{112}{2014}{{Woo et~al.}}{{Woo, Krishnan, and Wager}}}
\bibcite{Woolrich2001-tk}{{113}{2001}{{Woolrich et~al.}}{{Woolrich, Ripley, Brady, and Smith}}}
\bibcite{Woolrich2004-ng}{{114}{2004}{{Woolrich et~al.}}{{Woolrich, Behrens, Beckmann, Jenkinson, and Smith}}}
\bibcite{Worsley2000-rb}{{115}{2000}{{Worsley et~al.}}{{Worsley, Liao, Grabove, Petre, Ha, and Evans}}}
\bibcite{Yeung2018-kr}{{116}{2018}{{Yeung}}{{}}}

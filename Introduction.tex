Since its inception at the end of the twentieth century, functional Magnetic Resonance Imaging (fMRI) has experienced a meteoric rise to become the primary tool for human brain mapping. While many forms of the technique exist, introduction of the particular method based on the Blood Oxygenization Level Dependant (BOLD) effect has ultimately been the catalyst in elevating fMRI to such stature within the neuroimaging community. Taking advantage of the magnetic properties of oxygen-rich red blood cells, BOLD fMRI measures changes in blood oxygenization alongside cerebral blood flow and volume as a proxy to identify brain areas where elevated neuronal activity has occurred in response to a stimulus. While the relationship between the BOLD effect and neuronal activity is complex and remains controversial, it is the unique attributes of BOLD fMRI -- in particular, its capacity for non-invasive recording of signals across the entire brain at a high spatial resolution -- that set the technique apart from other scanning methods.

However, BOLD fMRI is also a \textit{noisy} process. The MR signals researchers set out to measure during a scanning session are corrupted by artefacts from both the imaging hardware and the physiology of the participant. Examples of scanner noise include inhomogeneities of the magnetic field that can cause spatial distortion or blurring in the MR image, and scanner drift characterized by temporal degradation of the signal. Physiological noise induced by subject motion, respiration, and heartbeat exacerbate the problem. 

Because of the low signal-to-noise, researchers must apply a series of statistical techniques to find meaning in the data. This usually entails carrying out a number of preprocessing, modelling and analysis steps that together constitute the fMRI processing pipeline. The fundamental objectives of preprocessing are to standardize brain locations across participants, to apply methods ensuring that the data conform to statistical assumptions required for analysis, and to reduce the influence of the aforementioned noise artefacts present in the data. This is achieved by conducting a number of steps, including slice-timing correction, motion correction, normalization, registration of the functional data to an anatomical template, and spatial smoothing. 

For task-based fMRI, a mass-univariate approach is utilized to model the data. During the scanning session, functional data are acquired in the form of voxels -- cubic intensity units that partition the brain comparable to the way in which pixels partition a computer screen. Each voxel's time-series is considered independently within the general linear model framework as a combination of signal components. To evaluate the effect of an experimental task condition relative to a baseline condition, hypothesis testing is performed at each voxel to compute a statistical parametric map of $t$-statistic values. Here, the behaviour of the signal under the null hypothesis of no activation is estimated using either a parametric approach, appealing to the body of mathematics known as Random Field Theory, or a nonparametric approach, where permutation methods are applied to estimate the null-distribution directly from the data. Finally, the statistical parametric map is thresholded to localize brain function. 

While we have provided a brief overview of the fMRI analysis pipeline, it is notable that there is not a general consensus as to how each particular analysis step should be carried out. Consequently, researchers have the freedom to make many choices during an analysis, such as how much smoothing is applied to the data, or how the heamodynamic response of blood flow to active neuronal tissues is modelled. However, this `methodological plurality' comes with a drawback. While conceptually similar, two different analysis pipelines applied on the same dataset may not produce the same scientific results, and mathematical modelling has shown that the high analytic flexibility associated with fMRI can potentially distort the final scientific findings of an investigation (Ioannidis, 2005, why most research findings are false). The problem is, with so many statistically valid methodological strategies available, if you try them all you are likely to find \textit{something}. Combined with further issues such as $p$-hacking and publication bias -- where there has been evidence to suggest that studies finding a significant effect are disproportionately represented in the fMRI literature -- these conditions have created the perfect storm: In recent years, many attempts to replicate the results of published fMRI studies have been unsuccessful, in what has been deemed as an ongoing reproducibility crisis within the field. 

The degree to which varying methodological decisions can lead to discrepancies in observed results has been investigated extensively. Choices for each individual procedure in the analysis pipeline (for example, head-motion regression (Lund et al., 2005), temporal filtering (Skudlarski et al., 1999), and autocorrelation correction (Woolrich et al., 2001)) alongside the order in which these procedures are conducted (Carp, 2013) can all deeply influence the final determined areas of brain activation. In perhaps the most comprehensive of such studies (Carp, 2012a), a single publicly available fMRI dataset was analyzed using over 6,000 unique analysis pipelines, generating 34,560 unique thresholded activation images. These results displayed a substantial degree of flexibility in both the sizes and locations of significant activation.

Alongside issues concerned with the flexibility of the analysis workflow, the statistical procedures carried out for fMRI inference have also come under intense scrutiny. Because statistical tests are conducted at each brain voxel independently, the $p$-values used to threshold the statistical parametric map are corrected to account for the large number of simultaneous comparisons being carried out and limit the expected number of voxels falsely declared as significant. This is almost always done using a false discovery rate correction procedure or a Bonferonni correction to limit the family-wise error rate of making at least one significant finding. 

The importance of such statistical correction methods were made prominent within the neuroimaging community using a humorous example, where one author identified significant activation in the brain of a dead salmon after applying inference with uncorrected $p$-values. However, in recent times they have been a source of major controversy. In 2016, a shocking paper by \textit{Eklund et. al} discovered that many fMRI software packages were incorrectly carrying out the multiple-correction procedures for clusterwise inferences, inflating the false-positive rate to up to 70\%. In a damning blow to the field, the implications of this study brought into question the validity of thousands of published fMRI results. 

While the relevant software packages have now been patched, deeper conceptual problems have been raised regarding the fMRI approach to inference. Specifically, there is a considerable amount of information that is \textit{not} captured when applying inference using cluster-size. In this setting, a significant p-value only indicates that a cluster is larger than expected by chance, and although a significant cluster may have a large spatial extent, since we can only infer that at least one voxel in the cluster has statistically significant signal, spatial specificity is low (Choong-Wan Woo, Cluster-extent based). In addition, this method does not provide a measure of the spatial variation of significant clusters. For illustration, imagine that a single fMRI study is repeated using two varying cohorts of participants; whereas we would expect moderate differences in the size and shape of clusters within each cohort's group-level thresholded map, current statistical results do not characterize this variability. 

A more pressing issue stems from an age-old paradox caused by the fallacy of the null hypothesis (Rozeboom, 1960). The paradox is that while the statistical models used for fMRI conventionally assume mean-zero noise, in reality all sources of noise will \textit{never} completely cancel. Therefore, improvements in experimental design will eventually lead to statistically significant results, and the null-hypothesis will, eventually, \textit{always} be rejected (Meehl, 1967). The recent availability of ambitious, large-sample studies (e.g Human Connectome Project (HCP), N = 1,200; UK Biobank, N = 30,000 and counting) have exemplified this problem. Analysis of high-quality fMRI data acquired under optimal noise conditions has been shown to display almost universal activation across the entire brain after hypothesis testing, even with stringent correction (Gonzalez-Castillo). For these reasons, there is an increased urgency for methods that can provide meaningful inference to interpret all significant effects. 

\bigskip

\noindent In this work, we aim to make contributions in two thematic areas currently challenging the field of task-based fMRI: Firstly, the need for further transparency to the degree in which the body of work comprising the fMRI literature is reproducible. Secondly, the need for further statistical methods to improve current inference practices carried out within the field. To achieve this, this dissertation is organized into four distinct chapters: Chapter 2 is dedicated to presenting the context of this work and providing background on the current methodological procedures carried out for analysis of task-fMRI data, with a particular emphasis on the statistical inference methods relevant to this thesis. In Chapter 3, we assess the analytic variability of group-level task-fMRI results under the choice of software package through which the analysis is conducted. In Chapter 4, we develop an inference method originally proposed for application on geospatial data in order to create spatial confidence sets on clusters found in fMRI percentage BOLD effect size maps. Finally, in Chapter 5 we make further theoretical developments to the inference method for application on the Cohen's $d$ and partial $R^{2}$ effect sizes commonly used in a task-fMRI study.  

%The variability of such methodological decisions is high between %published studies in the fMRI literature (Carp, 2012 (secret lives)).
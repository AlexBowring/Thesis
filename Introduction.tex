Since its inception at the end of the twentieth century, functional magnetic resonance imaging (fMRI) has experienced a meteoric rise to become the primary tool for human brain mapping. While many forms of the technique exist, introduction of the particular method based on the blood-oxygen-level-dependent (BOLD) effect has ultimately been the catalyst in elevating fMRI to such stature within the neuroimaging community. Taking advantage of the magnetic properties of oxygen-rich red blood cells, BOLD fMRI measures changes in blood oxygenization alongside cerebral blood flow and volume as a proxy to identify brain areas where elevated neuronal activity has occurred in response to a stimulus. While the relationship between the BOLD effect and neuronal activity is complex and remains controversial, it is the unique attributes of BOLD fMRI -- in particular, its capacity for non-invasive recording of signals across the entire brain at a high spatial resolution -- that set the technique apart from other scanning methods.

However, BOLD fMRI is also a noisy process. The magnetic resonance (MR) signals researchers set out to measure during a scanning session are corrupted by artefacts from both the imaging hardware and the physiology of the participant. Examples of scanner noise include inhomogeneities of the magnetic field that can cause spatial distortion or blurring in the MR image and scanner drift characterized by temporal degradation of the MR signal. Physiological noise induced by subject motion, respiration, and heartbeat exacerbate the problem. 

Because of the low signal-to-noise ratio, researchers must apply a series of statistical techniques to find meaning in the data. This usually entails carrying out a number of preprocessing, modelling and analysis steps that together constitute the fMRI processing pipeline. The fundamental objectives of preprocessing are to standardize brain locations across participants, to apply methods ensuring that the data conform to statistical assumptions required for analysis, and to reduce the influence of the noise artefacts present in the data. This is achieved by conducting a number of steps, including slice-timing correction, motion correction, normalization, registration of the functional data to an anatomical template, and spatial smoothing. 

For task-based fMRI, a mass-univariate approach is utilized to model the data. During the scanning session, functional data are acquired in the form of voxels -- cubic intensity units that partition the brain comparable to the way in which pixels partition a computer screen. Each voxel's time-series is considered independently within the general linear model framework as a combination of signal components. To evaluate the effect of an experimental task condition relative to a baseline condition, hypothesis testing is performed at each voxel to compute a statistical parametric map of statistic values. Here, the behaviour of the signal under the null-hypothesis of no activation is estimated using either a parametric approach, appealing to the body of mathematics known as Random Field Theory, or a nonparametric approach, where permutation methods are applied to estimate the null-distribution directly from the data. Finally, the statistical parametric map is thresholded to localize brain function. 

While we have provided a brief overview of the fMRI analysis pipeline, it is notable that there is not a general consensus as to how each particular analysis step should be carried out. Consequently, researchers have the freedom to make many choices during an analysis, such as how much smoothing is applied to the data, or how the haemodynamic response of blood flow to active neuronal tissues is modelled. However, this `methodological plurality' comes with a drawback. While conceptually similar, two different analysis pipelines applied on the same dataset may not produce the same scientific results; choice of analysis pipeline, operating system, and even software version can influence the final research outcomes of a study. Because of this, the high analytic flexibility associated with fMRI has been pinpointed as a key factor that can lead to distorted and irreproducible results \citep{Hong2019-qr,Ioannidis2005-sy,Wager2009-gm}. 

%The problem is, with so many statistically valid methodological strategies available, if you try them all you are likely to find \textit{something}. Combined with further issues such as $p$-hacking and publication bias -- where there has been evidence to suggest that studies finding a significant effect are disproportionately represented in the fMRI literature \citep{David2013-iz, Ioannidis2014-yn} -- these conditions have created the perfect storm: In recent years, many attempts to replicate the results of published fMRI studies have been unsuccessful, in what has been deemed as an ongoing reproducibility crisis within the field \citep{Poldrack2017-rr, Gorgolewski2016-fk, Open_Science_Collaboration2015-gr}. 

The degree to which varying methodological decisions can lead to discrepancies in observed results has been investigated extensively. Choices for each individual procedure in the analysis pipeline (for example, head-motion regression \citep{Lund2005-sf}, temporal filtering \citep{Skudlarski1999-ao}, and autocorrelation correction \citep{Woolrich2001-tk}) alongside the order in which these procedures are conducted \citep{Carp2013-cm} can all deeply influence the final determined areas of brain activation. In perhaps the most comprehensive of such studies \citep{Carp2012-ph}, a single publicly available fMRI dataset was analyzed using over 6,000 different pipelines, generating 34,560 unique thresholded activation images. These results displayed a substantial degree of flexibility in both the sizes and locations of significant activation. In combination, these examples of research shape a sombre picture for the possibility of study reproducibility. 

Alongside issues concerned with the flexibility of the analysis workflow, the statistical procedures carried out for fMRI inference have also come under intense scrutiny in recent times. Because statistical tests are conducted at each brain voxel independently, the $p$-value used to threshold the statistical parametric map is corrected to account for the large number of simultaneous comparisons being carried out and limit the expected number of voxels falsely declared as significant. This is almost always done using a false discovery rate correction procedure \citep{Benjamini1995-yy} or a Bonferonni correction to limit the familywise error rate of making at least one significant finding. 

The importance of such statistical correction methods were made prominent within the neuroimaging community using a humorous example, where one author identified significant activation in the brain of a dead salmon after applying inference with an uncorrected $p$-value \citep{Bennett2009-fh}. However, in recent times they have been a source of major controversy. In 2016, \citet*{Eklund2016-ak} discovered that many fMRI software packages were incorrectly carrying out the multiple-correction procedures for clusterwise inference, inflating the false-positive rate to beyond 40\%. The implications of this study brought into question the validity of thousands of published fMRI results, leading to updates in the main fMRI analysis packages \citep{Cox2017-wr} and a re-evaluation of how clusterwise inference should be applied \citep{Flandin2019-qx, Mueller2017-pn, Cox2017-ys}. 

Nevertheless, a number of conceptual limitations remain with the fMRI statistical approach to inference. Specifically, there is a considerable amount of information that is \textit{not} captured when applying inference using cluster-size. In this setting, a cluster-level $p$-value only conveys information about a cluster's spatial extent under the null-hypothesis. Since no information is provided regarding the statistical significance of each voxel comprising a significant cluster, the most we can say is that significant activation has occurred \textit{somewhere} inside the cluster \citep{Woo2014-ji}. An implication of this is that when a large, sprawling cluster covers many anatomical regions, the precise spatial specificity of the activation is in fact poor. A related problem of cluster inference is that no information is provided about the spatial variation of significant clusters. For example, if an fMRI study was to be repeated many times with new sets of subjects there would be variation in the size and shape of clusters found, yet the current statistical results have no way to characterize this variability.

A more pressing issue, perhaps, stems from an age-old paradox caused by the `fallacy of the null-hypothesis' \citep{Rozeboom1960-dp}. The paradox is that while statistical models conventionally assume mean-zero noise, in reality all sources of noise will never cancel, and therefore improvements in experimental design will eventually lead to statistically significant results. Thus, the null-hypothesis will, eventually, \textit{always} be rejected \citep{Meehl1967-ij}. The recent availability of ambitious, large-sample studies (e.g Human Connectome Project (HCP), N = 1,200; UK Biobank, N = 30,000 and counting) have exemplified this problem. Analysis of high-quality fMRI data acquired under optimal noise conditions has been shown to display almost universal activation across the entire brain after hypothesis testing, even with stringent correction \citep{Gonzalez-Castillo2012-do}. For these reasons, there is an increased urgency for methods that can provide meaningful inference to interpret all significant effects. 

\bigskip

\noindent In this work, we make contributions in two topical areas currently challenging the field of task-based fMRI: Firstly, the need for further transparency as to the degree in which the body of work comprising the fMRI literature is reproducible. Secondly, the need for further methods to improve current inference practices carried out within the field. To end this section, we summarize our main contributions before providing an outline of the organization of this thesis: 

\begin{enumerate}

\item While we have discussed a number of studies exploring how analysis decisions can influence the results of an fMRI investigation, for all of these studies the fundamental choice of software package through which the analysis was conducted remained constant. This is despite a vast array of packages that are used throughout the neuroimaging literature, the most popular of which are AFNI, FSL and SPM. In Chapter \ref{chap:software} we comprehensively assess how the choice of software package can impact the final results of a neuroimaging analysis. We reanalyze three published task-fMRI studies within AFNI, FSL and SPM, and quantify several aspects of variability between the three package's group-level statistical maps. Our findings suggest that exceedingly weak effects may not generalize across software. We are unaware of any comparable exercise in the literature.  

\item In carrying out the software comparison exercise, we implement a range of quantitative methods for the novel application of comparing fMRI statistical maps. These include Dice statistics, for assessing differences in the \textit{locations} of activation determined in each software's thresholded statistical results, Bland-Altman plots, for assessing differences between the \textit{magnitude} of statistic values in each software's unthresholded results, Euler Characteristics, for assessing differences in the topological properties of each software's activation profile, and Neurosynth analyses, for assessing differences in the anatomical regions associated to each software's activation pattern. We believe these methods are generalizable and hope they may benefit any further comparison of neuroimaging results. 

\item In Chapter \ref{chap:BOLD}, we develop an inference method originally proposed for application on geospatial data in \citet*{Sommerfeld2018-zl} (\textit{SSS}) to create spatial Confidence Sets (CSs) on clusters found in fMRI percentage BOLD effect size maps. While currently used hypothesis testing methods indicate where the null, i.e.\ an effect size of zero, can be rejected, this form of inference allows for statements about anatomical regions where effect sizes have exceeded, and fallen short of, a meaningful \textit{non-zero} threshold, such as areas where a BOLD change of 2.0\% has occurred. 

\item We make a number of theoretical and implementation advancements to the \textit{SSS} method for computing CSs that improve the method's finite-sample performance in the context of neuroimaging. In particular, we propose a combination of the Wild $t$-Bootstrap method and the use of Rademacher variables for multiplication of the bootstrapped residuals, which we find substantially improves performance of the method in moderate sample sizes. We also develop a linear interpolation method for computing the boundary over which the bootstrap is applied, and a novel approach for assessing the empirical coverage of the CSs that reduces upward bias in how the simulation results are measured.

\item In Chapter \ref{chap:cohen}, we make further theoretical developments to the CSs for application on standardized, Cohen's $d$ effect size images. By deriving the statistical properties of the Cohen's $d$ estimator, we motivate three separate algorithms to obtain Cohen's $d$ CSs. One of these methods is based on a novel procedure to normalize the distribution of the sample Cohen's $d$. By testing the three algorithms using intensive 3D Monte Carlo simulations, we conclude that two of the methods may perform particularly well on task-fMRI Cohen's $d$ effect size maps.

\end{enumerate}


\noindent The remainder of this thesis is organized into five chapters. 

\textbf{Chapter \ref{chap:background}} is dedicated to presenting the context of this work and providing background on the current methodological procedures carried out for analysis of task-fMRI data, with a particular emphasis on the statistical inference methods relevant to this thesis. 

In \textbf{Chapter \ref{chap:software}}, we assess the analytic variability of group-level task-fMRI results under the choice of software package through which the analysis is conducted. We reanalyze three published task-fMRI studies whose data has been made publicly available, attempting to replicate the original analysis procedures within each software package. We then make a number of qualitative and quantitative comparisons to assess the similarity of our results. 

In \textbf{Chapter \ref{chap:BOLD}}, we develop the inference method originally proposed in \textit{SSS} to create spatial Confidence Sets on clusters found in fMRI percentage BOLD effect size maps. We summarize the theory in \textit{SSS} before detailing our proposed modifications. We then carry out intensive Monte Carlo simulations to investigate the performance of the CSs on synthetic 3D signals representative of clusters found in fMRI effect size maps. Finally, we illustrate the method by computing CSs on 80 subject's percentage BOLD data from the Human Connectome Project working memory task.


In \textbf{Chapter \ref{chap:cohen}}, we make further theoretical developments to adapt the CSs for application to Cohen's $d$ effect size images. We derive the finite and asymptotic properties of the Cohen's $d$ estimator before formalizing three separate algorithms to compute Cohen's $d$ CSs. We assess the performance of the three methods using Monte Carlo simulations similarly to the previous chapter. Finally, we provide a demonstration of the three methods on Cohen's $d$ effect size maps from the Human Connectome Project dataset, comparing the Cohen's $d$ CSs to results obtained using a traditional statistical inference procedure.    

In \textbf{Chapter \ref{chap:conclusion}} we conclude this thesis, summarizing our contributions and providing discussion of possibilities for future work. 

%The variability of such methodological decisions is high between %published studies in the fMRI literature (Carp, 2012 (secret lives)).
Since its inception at the end of the twentieth century, functional Magnetic Resonance Imaging (fMRI) has experienced a meteoric rise to become the primary tool for human brain mapping. While many forms of the technique exist, introduction of the particular method based on the Blood Oxygenization Level Dependant (BOLD) effect has ultimately been the catalyst in elevating fMRI to such stature within the neuroimaging community. Taking advantage of the magnetic properties of oxygen-rich red blood cells, BOLD fMRI measures changes in blood oxygenization alongside cerebral blood flow and volume as a proxy to identify brain areas where elevated neuronal activity has occurred in response to a stimulus. While the relationship between the BOLD effect and neuronal activity is complex and remains controversial, it is the unique attributes of BOLD fMRI -- in particular, its capacity for non-invasive recording of signals across the entire brain at a high spatial resolution -- that set the technique apart from other scanning methods.

However, BOLD fMRI is also a \textit{noisy} process. The MR signals researchers set out to measure during a scanning session are corrupted by artefacts from both the imaging hardware and the physiology of the participant. Examples of scanner noise include inhomogeneities of the magnetic field that can cause spatial distortion or blurring in the MR image, and scanner drift characterized by temporal degradation of the signal. Physiological noise induced by subject motion, respiration, and heartbeat exacerbate the problem. 

Because of the low signal-to-noise, researchers must apply a series of statistical techniques to find meaning in the data. This usually entails carrying out a number of preprocessing, modelling and analysis steps that together constitute the fMRI processing pipeline. The fundamental objectives of preprocessing are to standardize brain locations across participants, to apply methods ensuring that the data conform to statistical assumptions required for analysis, and to reduce the influence of the aforementioned noise artefacts present in the data. This is achieved by conducting a number of steps, including slice-timing correction, motion correction, normalization, registration of the functional data to an anatomical template, and spatial smoothing. 

For task-based fMRI, a mass-univariate approach is utilized to model the data. During the scanning session, functional data are acquired in the form of voxels -- cubic intensity units that partition the brain comparable to the way in which pixels partition a computer screen. Each voxel's time-series is considered independently within the general linear model framework as a combination of signal components. To evaluate the effect of an experimental task condition relative to a baseline condition, hypothesis testing is performed at each voxel to compute a statistical parametric map of $t$-statistic values. Here, the behaviour of the signal under the null hypothesis of no activation is estimated using either a parametric approach, appealing to the body of mathematics known as Random Field Theory, or a nonparametric approach, where permutation methods are applied to estimate the null-distribution directly from the data. Finally, the statistical parametric map is thresholded to localize brain function. 

While we have provided a brief overview of the fMRI analysis pipeline, it is notable that there is not a general consensus as to how each particular analysis step should be carried out. Consequently, researchers have the freedom to make many choices during an analysis, such as how much smoothing is applied to the data, or how the heamodynamic response of blood flow to active neuronal tissues is modelled. However, this `methodological plurality' comes with a drawback. While conceptually similar, two different analysis pipelines applied on the same dataset may not produce the same scientific results, and mathematical modelling has shown that the high analytic flexibility associated with fMRI can potentially distort the final scientific findings of an investigation (Ioannidis, 2005, why most research findings are false). The problem is, with so many statistically valid methodological strategies available, if you try them all you are likely to find \textit{something}. Combined with further issues such as $p$-hacking and publication bias -- where there has been evidence to suggest that studies finding a significant effect are disproportionately represented in the fMRI literature -- these conditions have created the perfect storm: In recent years, many attempts to replicate the results of published fMRI studies have been unsuccessful, in what has been deemed as an ongoing reproducibility crisis within the field. 

The degree to which varying methodological decisions can lead to discrepancies in observed results has been investigated extensively. Choices for each individual procedure in the analysis pipeline (for example, head-motion regression (Lund et al., 2005), temporal filtering (Skudlarski et al., 1999), and autocorrelation correction (Woolrich et al., 2001)) alongside the order in which these procedures are conducted (Carp, 2013) can all deeply influence the final determined areas of brain activation. In perhaps the most comprehensive of such studies (Carp, 2012a), a single publicly available fMRI dataset was analyzed using over 6,000 unique analysis pipelines, generating 34,560 unique thresholded activation images. These results displayed a substantial degree of flexibility in both the sizes and locations of significant activation.

Alongside issues concerned with the flexibility of the analysis workflow, the statistical procedures carried out for fMRI inference have also come under intense scrutiny. Because statistical tests are conducted at each brain voxel independently, the $p$-values used to threshold the statistical parametric map are corrected to account for the large number of simultaneous comparisons being carried out and limit the expected number of voxels falsely declared as significant. This is almost always done using a false discovery rate correction procedure or a Bonferonni correction to limit the family-wise error rate of making at least one significant finding. 

The importance of such statistical correction methods were made prominent within the neuroimaging community using a humorous example, where one author identified significant activation in the brain of a dead salmon after applying inference with uncorrected $p$-values. However, in recent times they have been a source of major controversy. In 2016, a shocking paper by \textit{Eklund et. al} discovered that many fMRI software packages were incorrectly carrying out the multiple-correction procedures for clusterwise inferences, inflating the false-positive rate to up to 70\%. In a damning blow to the field, the implications of this study brought into question the validity of thousands of published fMRI results. 

While the relevant software packages have now been patched, deeper conceptual problems have been raised regarding the fMRI approach to inference. Specifically, there is a considerable amount of information that is \textit{not} captured when applying inference using cluster-size. In this setting, a significant p-value only indicates that a cluster is larger than expected by chance, and although a significant cluster may have a large spatial extent, since we can only infer that at least one voxel in the cluster has statistically significant signal, spatial specificity is low (Choong-Wan Woo, Cluster-extent based). In addition, this method does not provide a measure of the spatial variation of significant clusters. For illustration, imagine that a single fMRI study is repeated using two varying cohorts of participants; whereas we would expect moderate differences in the size and shape of clusters within each cohort's group-level thresholded map, current statistical results do not characterize this variability. 

A more pressing issue stems from an age-old paradox caused by the fallacy of the null hypothesis (Rozeboom, 1960). The paradox is that while the statistical models used for fMRI conventionally assume mean-zero noise, in reality all sources of noise will \textit{never} completely cancel. Therefore, improvements in experimental design will eventually lead to statistically significant results, and the null-hypothesis will, eventually, \textit{always} be rejected (Meehl, 1967). The recent availability of ambitious, large-sample studies (e.g Human Connectome Project (HCP), N = 1,200; UK Biobank, N = 30,000 and counting) have exemplified this problem. Analysis of high-quality fMRI data acquired under optimal noise conditions has been shown to display almost universal activation across the entire brain after hypothesis testing, even with stringent correction (Gonzalez-Castillo). For these reasons, there is an increased urgency for methods that can provide meaningful inference to interpret all significant effects. 

\bigskip

\noindent In this work, we make contributions in two thematic areas currently challenging the field of task-based fMRI: Firstly, the need for further transparency to the degree in which the body of work comprising the fMRI literature is reproducible. Secondly, the need for further statistical methods to improve current inference practices carried out within the field. To end this section, we summarize our main contributions before providing an outline of the organisation of this dissertation: 

\begin{enumerate}

\item While we have already discussed a number of studies exploring how decisions made at each stage of the analysis pipeline can influence the final scientific results of an fMRI investigation, for all of these studies the fundamental decision of which analysis software package the pipeline was conducted through remained constant. This is despite a vast array of analysis packages that are used throughout the neuroimaging literature, the most popular of which are AFNI, FSL and SPM. Motivated by this, in Chapter 2 we comprehensively assess how each of these software packages can impact analysis results by reanalyzing three published task-fMRI neuroimaging studies and quantifying several aspects of variability between the three package's group-level statistical maps. Our findings suggest that exceedingly weak effects may not generalise across software. We are unaware of any comparable exercise in the literature.  

\item In carrying out this software comparison exercise, we implement a range of quantitative methods for the novel application of comparing fMRI statistical maps. These include Dice statistic comparisons, for assessing differences in the determined regions of activation between the three software's thresholded statistical maps, Bland-Altman plots, for assessing differences between the magnitude of the $t$-static values in the unthresholded maps, Euler Characteristics, for assessing differences in the topological properties of each software's activation profile, and Neurosynth analyses, for assessing differences in the anatomical regions associated to each software's activation pattern. We believe these methods are generalizable and hope they may benefit any further comparison of neuroimaging results. 

\item In Chapter 3, we develop an inference method originally proposed for application on geospatial data in \textit{Sommerfeld, Sain, Schwartzman (2018)} \textit{SSS} to create spatial confidence sets on clusters found in fMRI percentage BOLD effect size maps. While currently used hypothesis testing methods indicate where the null, i.e. an effect size of zero, can be rejected, this form of inference allows for statements about anatomical regions where effect sizes have exceeded, and fallen short of, a \textit{non-zero} threshold, such as areas where a BOLD change of 2.0\% has occurred. 

\item In developing the inference method proposed by \textit{SSS}, we make theoretical advancements that improve the performance of the confidence sets, particularly for 3D data with moderate sample sizes. We also find that the methods used to assess the empirical coverage for simulations presented in \textit{SSS} are positively biased. We develop our own weighted-interpolation method for assessing empirical coverage, and on using this method, our simulation results validate the asymptotical mathematical theory set out in \textit{SSS}. 

\item In Chapter 4, we make further theoretical advancements to the confidence sets for application on the Cohen's $d$ and partial $R^{2}$ effect sizes.

\end{enumerate}


This dissertation is organized into five chapters: Chapter 2 is dedicated to presenting the context of this work and providing background on the current methodological procedures carried out for analysis of task-fMRI data, with a particular emphasis on the statistical inference methods relevant to this thesis. In Chapter 3, we assess the analytic variability of group-level task-fMRI results under the choice of software package through which the analysis is conducted. We reanalyze three published task-fMRI studies whose data has been made publicly available, attempting to replicate the original analysis procedure within each software package. We then make a number of comparisons to assess the similarity of our results. In Chapter 4, we develop the inference method originally proposed in \textit{SSS} to create spatial confidence sets on clusters found in fMRI percentage BOLD effect size maps. We summarize the theory in \textit{SSS} before detailing our proposed modifications. We then carry out intensive Monte Carlo simulations to investigate the accuracy of the confidence sets on synthetic 3D signals representative of clusters found in fMRI effect size maps. Futhermore, we illustrate the method by computing confidence sets on 80 subject's percentage BOLD data from the Human Connectome Project working memory task.
In Chapter 5 we make further theoretical developments to the inference method for application on the Cohen's $d$ and partial $R^{2}$ effect sizes commonly used in a task-fMRI study. Finally, in Chapter 6 we conclude this dissertation and provide further discussion of possibilities for future work. 

%The variability of such methodological decisions is high between %published studies in the fMRI literature (Carp, 2012 (secret lives)).